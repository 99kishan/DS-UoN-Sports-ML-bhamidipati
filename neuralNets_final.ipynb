{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import shap \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/kisha/Documents/Uni-Stuff/Dissertation/books/git/data_cleaned_no_weight.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Winner'])\n",
    "y= df['Winner']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4411, 145)\n"
     ]
    }
   ],
   "source": [
    "#checking number of features\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "3235    1\n",
      "82      0\n",
      "4081    1\n",
      "1030    1\n",
      "3892    0\n",
      "Name: Winner, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.dtype)\n",
    "print(y_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kisha\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kisha\\AppData\\Local\\Temp\\ipykernel_89916\\3134071260.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "C:\\Users\\kisha\\AppData\\Local\\Temp\\ipykernel_89916\\3134071260.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "#neural network\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_encoded, dtype=torch.float32).unsqueeze(1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.6500\n",
      "Epoch 2/50, Loss: 0.5717\n",
      "Epoch 3/50, Loss: 0.5167\n",
      "Epoch 4/50, Loss: 0.4819\n",
      "Epoch 5/50, Loss: 0.4564\n",
      "Epoch 6/50, Loss: 0.4295\n",
      "Epoch 7/50, Loss: 0.4091\n",
      "Epoch 8/50, Loss: 0.3823\n",
      "Epoch 9/50, Loss: 0.3636\n",
      "Epoch 10/50, Loss: 0.3483\n",
      "Epoch 11/50, Loss: 0.3289\n",
      "Epoch 12/50, Loss: 0.3105\n",
      "Epoch 13/50, Loss: 0.2907\n",
      "Epoch 14/50, Loss: 0.2688\n",
      "Epoch 15/50, Loss: 0.2568\n",
      "Epoch 16/50, Loss: 0.2546\n",
      "Epoch 17/50, Loss: 0.2246\n",
      "Epoch 18/50, Loss: 0.2132\n",
      "Epoch 19/50, Loss: 0.2088\n",
      "Epoch 20/50, Loss: 0.1932\n",
      "Epoch 21/50, Loss: 0.1776\n",
      "Epoch 22/50, Loss: 0.1657\n",
      "Epoch 23/50, Loss: 0.1461\n",
      "Epoch 24/50, Loss: 0.1465\n",
      "Epoch 25/50, Loss: 0.1500\n",
      "Epoch 26/50, Loss: 0.1225\n",
      "Epoch 27/50, Loss: 0.1153\n",
      "Epoch 28/50, Loss: 0.1026\n",
      "Epoch 29/50, Loss: 0.1159\n",
      "Epoch 30/50, Loss: 0.1099\n",
      "Epoch 31/50, Loss: 0.0981\n",
      "Epoch 32/50, Loss: 0.0930\n",
      "Epoch 33/50, Loss: 0.0791\n",
      "Epoch 34/50, Loss: 0.0752\n",
      "Epoch 35/50, Loss: 0.0670\n",
      "Epoch 36/50, Loss: 0.0662\n",
      "Epoch 37/50, Loss: 0.0788\n",
      "Epoch 38/50, Loss: 0.0791\n",
      "Epoch 39/50, Loss: 0.0651\n",
      "Epoch 40/50, Loss: 0.0680\n",
      "Epoch 41/50, Loss: 0.0565\n",
      "Epoch 42/50, Loss: 0.0536\n",
      "Epoch 43/50, Loss: 0.0528\n",
      "Epoch 44/50, Loss: 0.0547\n",
      "Epoch 45/50, Loss: 0.0690\n",
      "Epoch 46/50, Loss: 0.0695\n",
      "Epoch 47/50, Loss: 0.0592\n",
      "Epoch 48/50, Loss: 0.0509\n",
      "Epoch 49/50, Loss: 0.0689\n",
      "Epoch 50/50, Loss: 0.0698\n"
     ]
    }
   ],
   "source": [
    "input_size = 145  \n",
    "model1 = Net1(input_size)\n",
    "\n",
    "criterion = nn.BCELoss()  \n",
    "optimizer = optim.Adam(model1.parameters(), lr=0.001) \n",
    "\n",
    "num_epochs = 50  \n",
    "model1.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model1(inputs) \n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "\n",
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "    outputs1 = model1(X_test)\n",
    "    predictions1 = (outputs1.numpy() > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6155938349954669\n",
      "Precision: 0.4639443961772372\n",
      "Recall: 0.44911690496215306\n",
      "F1 Score: 0.4564102564102564\n",
      "ROC AUC Score: 0.5790395845565482\n"
     ]
    }
   ],
   "source": [
    "#evaluating the model\n",
    "print('Accuracy:', accuracy_score(y_test, predictions1))\n",
    "print('Precision:', precision_score(y_test, predictions1))\n",
    "print('Recall:', recall_score(y_test, predictions1))\n",
    "print('F1 Score:', f1_score(y_test, predictions1))\n",
    "print('ROC AUC Score:', roc_auc_score(y_test, predictions1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of y_test: 3309\n",
      "Length of predictions1: 3309\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of y_test: {len(y_test)}')\n",
    "print(f'Length of predictions1: {len(predictions1)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1503  617]\n",
      " [ 655  534]]\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, predictions1)\n",
    "\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oversampling minority class\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.45, random_state=42)\n",
    "\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.6517\n",
      "Epoch 2/50, Loss: 0.5717\n",
      "Epoch 3/50, Loss: 0.5236\n",
      "Epoch 4/50, Loss: 0.4844\n",
      "Epoch 5/50, Loss: 0.4575\n",
      "Epoch 6/50, Loss: 0.4364\n",
      "Epoch 7/50, Loss: 0.4092\n",
      "Epoch 8/50, Loss: 0.3933\n",
      "Epoch 9/50, Loss: 0.3675\n",
      "Epoch 10/50, Loss: 0.3375\n",
      "Epoch 11/50, Loss: 0.3365\n",
      "Epoch 12/50, Loss: 0.3042\n",
      "Epoch 13/50, Loss: 0.2851\n",
      "Epoch 14/50, Loss: 0.2663\n",
      "Epoch 15/50, Loss: 0.2538\n",
      "Epoch 16/50, Loss: 0.2358\n",
      "Epoch 17/50, Loss: 0.2288\n",
      "Epoch 18/50, Loss: 0.2111\n",
      "Epoch 19/50, Loss: 0.1949\n",
      "Epoch 20/50, Loss: 0.1778\n",
      "Epoch 21/50, Loss: 0.1670\n",
      "Epoch 22/50, Loss: 0.1598\n",
      "Epoch 23/50, Loss: 0.1429\n",
      "Epoch 24/50, Loss: 0.1422\n",
      "Epoch 25/50, Loss: 0.1325\n",
      "Epoch 26/50, Loss: 0.1237\n",
      "Epoch 27/50, Loss: 0.1135\n",
      "Epoch 28/50, Loss: 0.0994\n",
      "Epoch 29/50, Loss: 0.0887\n",
      "Epoch 30/50, Loss: 0.0822\n",
      "Epoch 31/50, Loss: 0.0807\n",
      "Epoch 32/50, Loss: 0.0779\n",
      "Epoch 33/50, Loss: 0.0671\n",
      "Epoch 34/50, Loss: 0.0708\n",
      "Epoch 35/50, Loss: 0.0791\n",
      "Epoch 36/50, Loss: 0.0756\n",
      "Epoch 37/50, Loss: 0.0838\n",
      "Epoch 38/50, Loss: 0.0744\n",
      "Epoch 39/50, Loss: 0.0711\n",
      "Epoch 40/50, Loss: 0.0615\n",
      "Epoch 41/50, Loss: 0.0553\n",
      "Epoch 42/50, Loss: 0.0483\n",
      "Epoch 43/50, Loss: 0.0394\n",
      "Epoch 44/50, Loss: 0.0527\n",
      "Epoch 45/50, Loss: 0.0639\n",
      "Epoch 46/50, Loss: 0.0837\n",
      "Epoch 47/50, Loss: 0.0520\n",
      "Epoch 48/50, Loss: 0.0436\n",
      "Epoch 49/50, Loss: 0.0537\n",
      "Epoch 50/50, Loss: 0.0502\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "input_size = 145  \n",
    "model2 = Net2(input_size)\n",
    "\n",
    "criterion = nn.BCELoss()  \n",
    "optimizer = optim.Adam(model2.parameters(), lr=0.001) \n",
    "\n",
    "\n",
    "num_epochs = 50  \n",
    "model2.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model2(inputs)  \n",
    "        loss = criterion(outputs, labels)  \n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    outputs2 = model2(X_test)\n",
    "    predictions2 = (outputs2.numpy() > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6013901480809912\n",
      "Precision: 0.44921875\n",
      "Recall: 0.48359966358284273\n",
      "F1 Score: 0.46577561765897124\n",
      "ROC AUC Score: 0.575526246885761\n"
     ]
    }
   ],
   "source": [
    "#evaluating the model\n",
    "print('Accuracy:', accuracy_score(y_test, predictions2))\n",
    "print('Precision:', precision_score(y_test, predictions2))\n",
    "print('Recall:', recall_score(y_test, predictions2))\n",
    "print('F1 Score:', f1_score(y_test, predictions2))\n",
    "print('ROC AUC Score:', roc_auc_score(y_test, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1415  705]\n",
      " [ 614  575]]\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, predictions2)\n",
    "\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kisha\\AppData\\Local\\Temp\\ipykernel_89916\\1189486903.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_features = torch.tensor(X_train, dtype=torch.float32)\n",
      "C:\\Users\\kisha\\AppData\\Local\\Temp\\ipykernel_89916\\1189486903.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_labels = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Correct placement of unsqueeze\n",
      "C:\\Users\\kisha\\AppData\\Local\\Temp\\ipykernel_89916\\1189486903.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_features = torch.tensor(X_test, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "train_features = torch.tensor(X_train, dtype=torch.float32)\n",
    "train_labels = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Correct placement of unsqueeze\n",
    "test_features = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset_unique = TensorDataset(train_features, train_labels)\n",
    "train_loader_unique = DataLoader(train_dataset_unique, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.6580\n",
      "Epoch 2/50, Loss: 0.5677\n",
      "Epoch 3/50, Loss: 0.5094\n",
      "Epoch 4/50, Loss: 0.4615\n",
      "Epoch 5/50, Loss: 0.4229\n",
      "Epoch 6/50, Loss: 0.4023\n",
      "Epoch 7/50, Loss: 0.3680\n",
      "Epoch 8/50, Loss: 0.3269\n",
      "Epoch 9/50, Loss: 0.2991\n",
      "Epoch 10/50, Loss: 0.2797\n",
      "Epoch 11/50, Loss: 0.2507\n",
      "Epoch 12/50, Loss: 0.2197\n",
      "Epoch 13/50, Loss: 0.2008\n",
      "Epoch 14/50, Loss: 0.1667\n",
      "Epoch 15/50, Loss: 0.1565\n",
      "Epoch 16/50, Loss: 0.1351\n",
      "Epoch 17/50, Loss: 0.1263\n",
      "Epoch 18/50, Loss: 0.1225\n",
      "Epoch 19/50, Loss: 0.0954\n",
      "Epoch 20/50, Loss: 0.0926\n",
      "Epoch 21/50, Loss: 0.0823\n",
      "Epoch 22/50, Loss: 0.0679\n",
      "Epoch 23/50, Loss: 0.0898\n",
      "Epoch 24/50, Loss: 0.0747\n",
      "Epoch 25/50, Loss: 0.0540\n",
      "Epoch 26/50, Loss: 0.0822\n",
      "Epoch 27/50, Loss: 0.0687\n",
      "Epoch 28/50, Loss: 0.0364\n",
      "Epoch 29/50, Loss: 0.0571\n",
      "Epoch 30/50, Loss: 0.1433\n",
      "Epoch 31/50, Loss: 0.0407\n",
      "Epoch 32/50, Loss: 0.0382\n",
      "Epoch 33/50, Loss: 0.0362\n",
      "Epoch 34/50, Loss: 0.0611\n",
      "Epoch 35/50, Loss: 0.0999\n",
      "Epoch 36/50, Loss: 0.0405\n",
      "Epoch 37/50, Loss: 0.0532\n",
      "Epoch 38/50, Loss: 0.0686\n",
      "Epoch 39/50, Loss: 0.0339\n",
      "Epoch 40/50, Loss: 0.0214\n",
      "Epoch 41/50, Loss: 0.0185\n",
      "Epoch 42/50, Loss: 0.0221\n",
      "Epoch 43/50, Loss: 0.0194\n",
      "Epoch 44/50, Loss: 0.0451\n",
      "Epoch 45/50, Loss: 0.0834\n",
      "Epoch 46/50, Loss: 0.0377\n",
      "Epoch 47/50, Loss: 0.0439\n",
      "Epoch 48/50, Loss: 0.0365\n",
      "Epoch 49/50, Loss: 0.0296\n",
      "Epoch 50/50, Loss: 0.0408\n"
     ]
    }
   ],
   "source": [
    "class DeepNet1(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepNet1, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.layer4 = nn.Linear(64, 32)\n",
    "        self.layer5 = nn.Linear(32, 16)\n",
    "        self.output_layer = nn.Linear(16, 1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.layer1(x))\n",
    "        x = self.activation(self.layer2(x))\n",
    "        x = self.activation(self.layer3(x))\n",
    "        x = self.activation(self.layer4(x))\n",
    "        x = self.activation(self.layer5(x))\n",
    "        x = self.output_activation(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "input_dim = 145\n",
    "deep_model1 = DeepNet1(input_dim)\n",
    "\n",
    "loss_function1 = nn.BCELoss()\n",
    "optimizer1 = optim.Adam(deep_model1.parameters(), lr=0.001)\n",
    "\n",
    "total_epochs = 50\n",
    "deep_model1.train()\n",
    "for epoch_num in range(total_epochs):\n",
    "    accumulated_loss1 = 0.0\n",
    "    for batch_features, batch_labels in train_loader_unique:\n",
    "        batch_labels = batch_labels.view(-1,1)  # Remove extra dimensions if any\n",
    "        optimizer1.zero_grad()\n",
    "        batch_outputs = deep_model1(batch_features)\n",
    "        batch_loss = loss_function1(batch_outputs, batch_labels)\n",
    "        batch_loss.backward()\n",
    "        optimizer1.step()\n",
    "\n",
    "        accumulated_loss1 += batch_loss.item() * batch_features.size(0)\n",
    "\n",
    "    avg_epoch_loss1 = accumulated_loss1 / len(train_loader_unique.dataset)\n",
    "    print(f'Epoch {epoch_num+1}/{total_epochs}, Loss: {avg_epoch_loss1:.4f}')\n",
    "\n",
    "deep_model1.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs1 = deep_model1(test_features)\n",
    "    test_predictions1 = (test_outputs1.numpy() > 0.5).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6288909035962527\n",
      "Precision: 0.4790996784565916\n",
      "Recall: 0.375946173254836\n",
      "F1 Score: 0.4213006597549482\n",
      "ROC AUC Score: 0.5733504451179839\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy_score(y_test, test_predictions1))\n",
    "print('Precision:', precision_score(y_test, test_predictions1))\n",
    "print('Recall:', recall_score(y_test, test_predictions1))\n",
    "print('F1 Score:', f1_score(y_test, test_predictions1))\n",
    "print('ROC AUC Score:', roc_auc_score(y_test, test_predictions1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1634  486]\n",
      " [ 742  447]]\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, test_predictions1)\n",
    "\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.6582\n",
      "Epoch 2/50, Loss: 0.5741\n",
      "Epoch 3/50, Loss: 0.5035\n",
      "Epoch 4/50, Loss: 0.4630\n",
      "Epoch 5/50, Loss: 0.4275\n",
      "Epoch 6/50, Loss: 0.3975\n",
      "Epoch 7/50, Loss: 0.3752\n",
      "Epoch 8/50, Loss: 0.3399\n",
      "Epoch 9/50, Loss: 0.3031\n",
      "Epoch 10/50, Loss: 0.2756\n",
      "Epoch 11/50, Loss: 0.2557\n",
      "Epoch 12/50, Loss: 0.2286\n",
      "Epoch 13/50, Loss: 0.1997\n",
      "Epoch 14/50, Loss: 0.1804\n",
      "Epoch 15/50, Loss: 0.1553\n",
      "Epoch 16/50, Loss: 0.1382\n",
      "Epoch 17/50, Loss: 0.1263\n",
      "Epoch 18/50, Loss: 0.1203\n",
      "Epoch 19/50, Loss: 0.1127\n",
      "Epoch 20/50, Loss: 0.0937\n",
      "Epoch 21/50, Loss: 0.0897\n",
      "Epoch 22/50, Loss: 0.0829\n",
      "Epoch 23/50, Loss: 0.0901\n",
      "Epoch 24/50, Loss: 0.0659\n",
      "Epoch 25/50, Loss: 0.0565\n",
      "Epoch 26/50, Loss: 0.0879\n",
      "Epoch 27/50, Loss: 0.0616\n",
      "Epoch 28/50, Loss: 0.0741\n",
      "Epoch 29/50, Loss: 0.0514\n",
      "Epoch 30/50, Loss: 0.0580\n",
      "Epoch 31/50, Loss: 0.0634\n",
      "Epoch 32/50, Loss: 0.0647\n",
      "Epoch 33/50, Loss: 0.0551\n",
      "Epoch 34/50, Loss: 0.0527\n",
      "Epoch 35/50, Loss: 0.0519\n",
      "Epoch 36/50, Loss: 0.0721\n",
      "Epoch 37/50, Loss: 0.0401\n",
      "Epoch 38/50, Loss: 0.0357\n",
      "Epoch 39/50, Loss: 0.0569\n",
      "Epoch 40/50, Loss: 0.0394\n",
      "Epoch 41/50, Loss: 0.0387\n",
      "Epoch 42/50, Loss: 0.0253\n",
      "Epoch 43/50, Loss: 0.0208\n",
      "Epoch 44/50, Loss: 0.0216\n",
      "Epoch 45/50, Loss: 0.0190\n",
      "Epoch 46/50, Loss: 0.0159\n",
      "Epoch 47/50, Loss: 0.0362\n",
      "Epoch 48/50, Loss: 0.0572\n",
      "Epoch 49/50, Loss: 0.0362\n",
      "Epoch 50/50, Loss: 0.0640\n"
     ]
    }
   ],
   "source": [
    "train_dataset_unique = TensorDataset(train_features, train_labels)\n",
    "train_loader_unique = DataLoader(train_dataset_unique, batch_size=64, shuffle=True)\n",
    "\n",
    "class DeepNet2(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepNet2, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.layer4 = nn.Linear(64, 32)\n",
    "        self.layer5 = nn.Linear(32, 16)\n",
    "        self.output_layer = nn.Linear(16, 1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.layer1(x))\n",
    "        x = self.activation(self.layer2(x))\n",
    "        x = self.activation(self.layer3(x))\n",
    "        x = self.activation(self.layer4(x))\n",
    "        x = self.activation(self.layer5(x))\n",
    "        x = self.output_activation(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "input_dim = 145\n",
    "deep_model2 = DeepNet2(input_dim)\n",
    "\n",
    "loss_function2 = nn.BCELoss()\n",
    "optimizer2 = optim.Adam(deep_model2.parameters(), lr=0.001)\n",
    "\n",
    "total_epochs = 50\n",
    "deep_model2.train()\n",
    "for epoch_num in range(total_epochs):\n",
    "    accumulated_loss2 = 0.0\n",
    "    for batch_features, batch_labels in train_loader_unique:\n",
    "        batch_labels = batch_labels.view(-1,1)  # Remove extra dimensions if any\n",
    "        optimizer2.zero_grad()\n",
    "        batch_outputs = deep_model2(batch_features)\n",
    "        batch_loss = loss_function2(batch_outputs, batch_labels)\n",
    "        batch_loss.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        accumulated_loss2 += batch_loss.item() * batch_features.size(0)\n",
    "\n",
    "    avg_epoch_loss2 = accumulated_loss2 / len(train_loader_unique.dataset)\n",
    "    print(f'Epoch {epoch_num+1}/{total_epochs}, Loss: {avg_epoch_loss2:.4f}')\n",
    "\n",
    "deep_model2.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs2 = deep_model2(test_features)\n",
    "    test_predictions2 = (test_outputs2.numpy() > 0.5).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6252644303414929\n",
      "Precision: 0.47335423197492166\n",
      "Recall: 0.3809924306139613\n",
      "F1 Score: 0.42218080149114634\n",
      "ROC AUC Score: 0.5716282907786788\n",
      "Confusion Matrix: [[1616  504]\n",
      " [ 736  453]]\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.76      0.72      2120\n",
      "           1       0.47      0.38      0.42      1189\n",
      "\n",
      "    accuracy                           0.63      3309\n",
      "   macro avg       0.58      0.57      0.57      3309\n",
      "weighted avg       0.61      0.63      0.61      3309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy_score(y_test, test_predictions2))\n",
    "print('Precision:', precision_score(y_test, test_predictions2))\n",
    "print('Recall:', recall_score(y_test, test_predictions2))\n",
    "print('F1 Score:', f1_score(y_test, test_predictions2))\n",
    "print('ROC AUC Score:', roc_auc_score(y_test, test_predictions2))\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, test_predictions2))\n",
    "print('Classification Report:', classification_report(y_test, test_predictions2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.7075\n",
      "Epoch 2/50, Loss: 0.6986\n",
      "Epoch 3/50, Loss: 0.6791\n",
      "Epoch 4/50, Loss: 0.6669\n",
      "Epoch 5/50, Loss: 0.6591\n",
      "Epoch 6/50, Loss: 0.6437\n",
      "Epoch 7/50, Loss: 0.6292\n",
      "Epoch 8/50, Loss: 0.6146\n",
      "Epoch 9/50, Loss: 0.5996\n",
      "Epoch 10/50, Loss: 0.5909\n",
      "Epoch 11/50, Loss: 0.5725\n",
      "Epoch 12/50, Loss: 0.5700\n",
      "Epoch 13/50, Loss: 0.5594\n",
      "Epoch 14/50, Loss: 0.5573\n",
      "Epoch 15/50, Loss: 0.5471\n",
      "Epoch 16/50, Loss: 0.5391\n",
      "Epoch 17/50, Loss: 0.5227\n",
      "Epoch 18/50, Loss: 0.5246\n",
      "Epoch 19/50, Loss: 0.5165\n",
      "Epoch 20/50, Loss: 0.5289\n",
      "Epoch 21/50, Loss: 0.5154\n",
      "Epoch 22/50, Loss: 0.5084\n",
      "Epoch 23/50, Loss: 0.5069\n",
      "Epoch 24/50, Loss: 0.4985\n",
      "Epoch 25/50, Loss: 0.4925\n",
      "Epoch 26/50, Loss: 0.4864\n",
      "Epoch 27/50, Loss: 0.4804\n",
      "Epoch 28/50, Loss: 0.4847\n",
      "Epoch 29/50, Loss: 0.4782\n",
      "Epoch 30/50, Loss: 0.4699\n",
      "Epoch 31/50, Loss: 0.4704\n",
      "Epoch 32/50, Loss: 0.4661\n",
      "Epoch 33/50, Loss: 0.4678\n",
      "Epoch 34/50, Loss: 0.4606\n",
      "Epoch 35/50, Loss: 0.4677\n",
      "Epoch 36/50, Loss: 0.4545\n",
      "Epoch 37/50, Loss: 0.4545\n",
      "Epoch 38/50, Loss: 0.4591\n",
      "Epoch 39/50, Loss: 0.4470\n",
      "Epoch 40/50, Loss: 0.4427\n",
      "Epoch 41/50, Loss: 0.4441\n",
      "Epoch 42/50, Loss: 0.4325\n",
      "Epoch 43/50, Loss: 0.4381\n",
      "Epoch 44/50, Loss: 0.4353\n",
      "Epoch 45/50, Loss: 0.4424\n",
      "Epoch 46/50, Loss: 0.4167\n",
      "Epoch 47/50, Loss: 0.4318\n",
      "Epoch 48/50, Loss: 0.4233\n",
      "Epoch 49/50, Loss: 0.4229\n",
      "Epoch 50/50, Loss: 0.4198\n"
     ]
    }
   ],
   "source": [
    "train_dataset_unique = TensorDataset(train_features, train_labels)\n",
    "train_loader_unique = DataLoader(train_dataset_unique, batch_size=64, shuffle=True)\n",
    "\n",
    "class DeepNet3(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepNet3, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.layer4 = nn.Linear(64, 32)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        self.layer5 = nn.Linear(32, 16)\n",
    "        self.bn5 = nn.BatchNorm1d(16)\n",
    "        self.output_layer = nn.Linear(16, 1)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.activation(self.bn1(self.layer1(x))))\n",
    "        x = self.dropout(self.activation(self.bn2(self.layer2(x))))\n",
    "        x = self.dropout(self.activation(self.bn3(self.layer3(x))))\n",
    "        x = self.dropout(self.activation(self.bn4(self.layer4(x))))\n",
    "        x = self.dropout(self.activation(self.bn5(self.layer5(x))))\n",
    "        x = self.output_activation(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "input_dim = 145\n",
    "deep_model3 = DeepNet3(input_dim)\n",
    "\n",
    "loss_function3 = nn.BCELoss()\n",
    "optimizer3 = optim.Adam(deep_model3.parameters(), lr=0.001)\n",
    "\n",
    "total_epochs = 50\n",
    "deep_model3.train()\n",
    "for epoch_num in range(total_epochs):\n",
    "    accumulated_loss3 = 0.0\n",
    "    for batch_features, batch_labels in train_loader_unique:\n",
    "        batch_labels = batch_labels.view(-1,1)  \n",
    "        optimizer3.zero_grad()\n",
    "        batch_outputs = deep_model3(batch_features)\n",
    "        batch_loss = loss_function3(batch_outputs, batch_labels)\n",
    "        batch_loss.backward()\n",
    "        optimizer3.step()\n",
    "\n",
    "        accumulated_loss3 += batch_loss.item() * batch_features.size(0)\n",
    "\n",
    "    avg_epoch_loss3 = accumulated_loss3 / len(train_loader_unique.dataset)\n",
    "    print(f'Epoch {epoch_num+1}/{total_epochs}, Loss: {avg_epoch_loss3:.4f}')\n",
    "\n",
    "deep_model3.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs3 = deep_model3(test_features)\n",
    "    test_predictions3 = (test_outputs3.numpy() > 0.5).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.642792384406165\n",
      "Precision: 0.5030355594102341\n",
      "Recall: 0.4878048780487805\n",
      "F1 Score: 0.49530315969257044\n",
      "ROC AUC Score: 0.6087609295904279\n",
      "Confusion Matrix: [[1547  573]\n",
      " [ 609  580]]\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.73      0.72      2120\n",
      "           1       0.50      0.49      0.50      1189\n",
      "\n",
      "    accuracy                           0.64      3309\n",
      "   macro avg       0.61      0.61      0.61      3309\n",
      "weighted avg       0.64      0.64      0.64      3309\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Accuracy:', accuracy_score(y_test, test_predictions3))\n",
    "print('Precision:', precision_score(y_test, test_predictions3))\n",
    "print('Recall:', recall_score(y_test, test_predictions3))\n",
    "print('F1 Score:', f1_score(y_test, test_predictions3))\n",
    "print('ROC AUC Score:', roc_auc_score(y_test, test_predictions3))\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, test_predictions3))\n",
    "print('Classification Report:', classification_report(y_test, test_predictions3))\n",
    "\n",
    "#confusion matrix\n",
    "cm2 = confusion_matrix(y_test, test_predictions3)\n",
    "sns.heatmap(cm2, annot=True, fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model\n",
    "torch.save(deep_model3.state_dict(), 'C:/Users/kisha/Documents/Uni-Stuff/Dissertation/books/git/DS-UoN-Sports-ML-bhamidipati/savedModels/deep_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kisha\\AppData\\Local\\Temp\\ipykernel_89916\\4242420197.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_features = torch.tensor(X_train, dtype=torch.float32)\n",
      "C:\\Users\\kisha\\AppData\\Local\\Temp\\ipykernel_89916\\4242420197.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_labels = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
      "C:\\Users\\kisha\\AppData\\Local\\Temp\\ipykernel_89916\\4242420197.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_features = torch.tensor(X_test, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.6688\n",
      "Epoch 2/50, Loss: 0.5791\n",
      "Epoch 3/50, Loss: 0.5244\n",
      "Epoch 4/50, Loss: 0.4826\n",
      "Epoch 5/50, Loss: 0.4489\n",
      "Epoch 6/50, Loss: 0.4183\n",
      "Epoch 7/50, Loss: 0.3943\n",
      "Epoch 8/50, Loss: 0.3576\n",
      "Epoch 9/50, Loss: 0.3347\n",
      "Epoch 10/50, Loss: 0.3133\n",
      "Epoch 11/50, Loss: 0.2766\n",
      "Epoch 12/50, Loss: 0.2746\n",
      "Epoch 13/50, Loss: 0.2390\n",
      "Epoch 14/50, Loss: 0.2150\n",
      "Epoch 15/50, Loss: 0.1776\n",
      "Epoch 16/50, Loss: 0.1644\n",
      "Epoch 17/50, Loss: 0.1548\n",
      "Epoch 18/50, Loss: 0.1418\n",
      "Epoch 19/50, Loss: 0.1263\n",
      "Epoch 20/50, Loss: 0.1117\n",
      "Epoch 21/50, Loss: 0.1241\n",
      "Epoch 22/50, Loss: 0.0945\n",
      "Epoch 23/50, Loss: 0.0918\n",
      "Epoch 24/50, Loss: 0.0860\n",
      "Epoch 25/50, Loss: 0.0860\n",
      "Epoch 26/50, Loss: 0.0856\n",
      "Epoch 27/50, Loss: 0.0695\n",
      "Epoch 28/50, Loss: 0.0720\n",
      "Epoch 29/50, Loss: 0.0683\n",
      "Epoch 30/50, Loss: 0.0610\n",
      "Epoch 31/50, Loss: 0.0724\n",
      "Epoch 32/50, Loss: 0.0500\n",
      "Epoch 33/50, Loss: 0.0574\n",
      "Epoch 34/50, Loss: 0.0501\n",
      "Epoch 35/50, Loss: 0.0489\n",
      "Epoch 36/50, Loss: 0.0463\n",
      "Epoch 37/50, Loss: 0.0567\n",
      "Epoch 38/50, Loss: 0.0499\n",
      "Epoch 39/50, Loss: 0.0320\n",
      "Epoch 40/50, Loss: 0.0414\n",
      "Epoch 41/50, Loss: 0.0352\n",
      "Epoch 42/50, Loss: 0.0428\n",
      "Epoch 43/50, Loss: 0.0387\n",
      "Epoch 44/50, Loss: 0.0516\n",
      "Epoch 45/50, Loss: 0.0329\n",
      "Epoch 46/50, Loss: 0.0362\n",
      "Epoch 47/50, Loss: 0.0490\n",
      "Epoch 48/50, Loss: 0.0313\n",
      "Epoch 49/50, Loss: 0.0382\n",
      "Epoch 50/50, Loss: 0.0317\n"
     ]
    }
   ],
   "source": [
    "train_features = torch.tensor(X_train, dtype=torch.float32)\n",
    "train_labels = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "test_features = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "if train_labels.dim() > 2:\n",
    "    train_labels = train_labels.squeeze(-1)\n",
    "\n",
    "train_dataset_unique = TensorDataset(train_features, train_labels)\n",
    "train_loader_unique = DataLoader(train_dataset_unique, batch_size=64, shuffle=True)\n",
    "\n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 512)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.layer4 = nn.Linear(128, 64)\n",
    "        self.layer5 = nn.Linear(64, 32)\n",
    "        self.layer6 = nn.Linear(32, 16)\n",
    "        self.layer7 = nn.Linear(16, 8)\n",
    "        self.output_layer = nn.Linear(8, 1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.layer1(x))\n",
    "        x = self.activation(self.layer2(x))\n",
    "        x = self.activation(self.layer3(x))\n",
    "        x = self.activation(self.layer4(x))\n",
    "        x = self.activation(self.layer5(x))\n",
    "        x = self.activation(self.layer6(x))\n",
    "        x = self.activation(self.layer7(x))\n",
    "        x = self.output_activation(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "input_dim = 145\n",
    "deep_model = DeepNet(input_dim)\n",
    "\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer_unique = optim.Adam(deep_model.parameters(), lr=0.001)\n",
    "\n",
    "total_epochs = 50\n",
    "deep_model.train()\n",
    "for epoch_num in range(total_epochs):\n",
    "    accumulated_loss = 0.0\n",
    "    for batch_features, batch_labels in train_loader_unique:\n",
    "        optimizer_unique.zero_grad()\n",
    "        batch_outputs = deep_model(batch_features)\n",
    "        batch_loss = loss_function(batch_outputs, batch_labels)\n",
    "        batch_loss.backward()\n",
    "        optimizer_unique.step()\n",
    "\n",
    "        accumulated_loss += batch_loss.item() * batch_features.size(0)\n",
    "\n",
    "    avg_epoch_loss = accumulated_loss / len(train_loader_unique.dataset)\n",
    "    print(f'Epoch {epoch_num+1}/{total_epochs}, Loss: {avg_epoch_loss:.4f}')\n",
    "\n",
    "deep_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = deep_model(test_features)\n",
    "    test_predictions = (test_outputs.numpy() > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6134783922635237\n",
      "Precision: 0.46017699115044247\n",
      "Recall: 0.4373423044575273\n",
      "F1 Score: 0.44846916774471757\n"
     ]
    }
   ],
   "source": [
    "#evaluating the model\n",
    "print('Accuracy:', accuracy_score(y_test, test_predictions))\n",
    "print('Precision:', precision_score(y_test, test_predictions))\n",
    "print('Recall:', recall_score(y_test, test_predictions))\n",
    "print('F1 Score:', f1_score(y_test, test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.6852\n",
      "Epoch 2/100, Loss: 0.6754\n",
      "Epoch 3/100, Loss: 0.6704\n",
      "Epoch 4/100, Loss: 0.6639\n",
      "Epoch 5/100, Loss: 0.6537\n",
      "Epoch 6/100, Loss: 0.6484\n",
      "Epoch 7/100, Loss: 0.6451\n",
      "Epoch 8/100, Loss: 0.6431\n",
      "Epoch 9/100, Loss: 0.6235\n",
      "Epoch 10/100, Loss: 0.5846\n",
      "Epoch 11/100, Loss: 0.5376\n",
      "Epoch 12/100, Loss: 0.5136\n",
      "Epoch 13/100, Loss: 0.4937\n",
      "Epoch 14/100, Loss: 0.4815\n",
      "Epoch 15/100, Loss: 0.4627\n",
      "Epoch 16/100, Loss: 0.4430\n",
      "Epoch 17/100, Loss: 0.4253\n",
      "Epoch 18/100, Loss: 0.4121\n",
      "Epoch 19/100, Loss: 0.3995\n",
      "Epoch 20/100, Loss: 0.3863\n",
      "Epoch 21/100, Loss: 0.3600\n",
      "Epoch 22/100, Loss: 0.3853\n",
      "Epoch 23/100, Loss: 0.3535\n",
      "Epoch 24/100, Loss: 0.3428\n",
      "Epoch 25/100, Loss: 0.3280\n",
      "Epoch 26/100, Loss: 0.3311\n",
      "Epoch 27/100, Loss: 0.3191\n",
      "Epoch 28/100, Loss: 0.3125\n",
      "Epoch 29/100, Loss: 0.2934\n",
      "Epoch 30/100, Loss: 0.2943\n",
      "Epoch 31/100, Loss: 0.2881\n",
      "Epoch 32/100, Loss: 0.2600\n",
      "Epoch 33/100, Loss: 0.2718\n",
      "Epoch 34/100, Loss: 0.2679\n",
      "Epoch 35/100, Loss: 0.2502\n",
      "Epoch 36/100, Loss: 0.2623\n",
      "Epoch 37/100, Loss: 0.2537\n",
      "Epoch 38/100, Loss: 0.2385\n",
      "Epoch 39/100, Loss: 0.2441\n",
      "Epoch 40/100, Loss: 0.2427\n",
      "Epoch 41/100, Loss: 0.2894\n",
      "Epoch 42/100, Loss: 0.2371\n",
      "Epoch 43/100, Loss: 0.2284\n",
      "Epoch 44/100, Loss: 0.2142\n",
      "Epoch 45/100, Loss: 0.2291\n",
      "Epoch 46/100, Loss: 0.2244\n",
      "Epoch 47/100, Loss: 0.2245\n",
      "Epoch 48/100, Loss: 0.2166\n",
      "Epoch 49/100, Loss: 0.2218\n",
      "Epoch 50/100, Loss: 0.2222\n",
      "Epoch 51/100, Loss: 0.2169\n",
      "Epoch 52/100, Loss: 0.2003\n",
      "Epoch 53/100, Loss: 0.1989\n",
      "Epoch 54/100, Loss: 0.2079\n",
      "Epoch 55/100, Loss: 0.2155\n",
      "Epoch 56/100, Loss: 0.2142\n",
      "Epoch 57/100, Loss: 0.1988\n",
      "Epoch 58/100, Loss: 0.2094\n",
      "Epoch 59/100, Loss: 0.2382\n",
      "Epoch 60/100, Loss: 0.1943\n",
      "Epoch 61/100, Loss: 0.1935\n",
      "Epoch 62/100, Loss: 0.1856\n",
      "Epoch 63/100, Loss: 0.2214\n",
      "Epoch 64/100, Loss: 0.1959\n",
      "Epoch 65/100, Loss: 0.2041\n",
      "Epoch 66/100, Loss: 0.1994\n",
      "Epoch 67/100, Loss: 0.1820\n",
      "Epoch 68/100, Loss: 0.1968\n",
      "Epoch 69/100, Loss: 0.1882\n",
      "Epoch 70/100, Loss: 0.2158\n",
      "Epoch 71/100, Loss: 0.2080\n",
      "Epoch 72/100, Loss: 0.2383\n",
      "Epoch 73/100, Loss: 0.1870\n",
      "Epoch 74/100, Loss: 0.1716\n",
      "Epoch 75/100, Loss: 0.1789\n",
      "Epoch 76/100, Loss: 0.2119\n",
      "Epoch 77/100, Loss: 0.2020\n",
      "Epoch 78/100, Loss: 0.1829\n",
      "Epoch 79/100, Loss: 0.1790\n",
      "Epoch 80/100, Loss: 0.2200\n",
      "Epoch 81/100, Loss: 0.1964\n",
      "Epoch 82/100, Loss: 0.1858\n",
      "Epoch 83/100, Loss: 0.1892\n",
      "Epoch 84/100, Loss: 0.2859\n",
      "Epoch 85/100, Loss: 0.1977\n",
      "Epoch 86/100, Loss: 0.1724\n",
      "Epoch 87/100, Loss: 0.1687\n",
      "Epoch 88/100, Loss: 0.1721\n",
      "Epoch 89/100, Loss: 0.1785\n",
      "Epoch 90/100, Loss: 0.1893\n",
      "Epoch 91/100, Loss: 0.1689\n",
      "Epoch 92/100, Loss: 0.1667\n",
      "Epoch 93/100, Loss: 0.1720\n",
      "Epoch 94/100, Loss: 0.1683\n",
      "Epoch 95/100, Loss: 0.1770\n",
      "Epoch 96/100, Loss: 0.1746\n",
      "Epoch 97/100, Loss: 0.1736\n",
      "Epoch 98/100, Loss: 0.1717\n",
      "Epoch 99/100, Loss: 0.1736\n",
      "Epoch 100/100, Loss: 0.1719\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 2048)\n",
    "        self.layer2 = nn.Linear(2048, 1024)\n",
    "        self.layer3 = nn.Linear(1024, 512)\n",
    "        self.layer4 = nn.Linear(512, 256)\n",
    "        self.layer5 = nn.Linear(256, 128)\n",
    "        self.layer6 = nn.Linear(128, 64)\n",
    "        self.layer7 = nn.Linear(64, 32)\n",
    "        self.layer8 = nn.Linear(32, 16)\n",
    "        self.layer9 = nn.Linear(16, 8)\n",
    "        self.layer10 = nn.Linear(8, 4)\n",
    "        self.layer11 = nn.Linear(4, 2)\n",
    "        self.output_layer = nn.Linear(2, 1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.layer1(x))\n",
    "        x = self.activation(self.layer2(x))\n",
    "        x = self.activation(self.layer3(x))\n",
    "        x = self.activation(self.layer4(x))\n",
    "        x = self.activation(self.layer5(x))\n",
    "        x = self.activation(self.layer6(x))\n",
    "        x = self.activation(self.layer7(x))\n",
    "        x = self.activation(self.layer8(x))\n",
    "        x = self.activation(self.layer9(x))\n",
    "        x = self.activation(self.layer10(x))\n",
    "        x = self.activation(self.layer11(x))\n",
    "        x = self.output_activation(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "input_dim = 145\n",
    "deep_model = DeepNet(input_dim)\n",
    "\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer_unique = optim.Adam(deep_model.parameters(), lr=0.001)\n",
    "\n",
    "total_epochs = 100\n",
    "deep_model.train()\n",
    "for epoch_num in range(total_epochs):\n",
    "    accumulated_loss = 0.0\n",
    "    for batch_features, batch_labels in train_loader_unique:\n",
    "        optimizer_unique.zero_grad()\n",
    "        batch_outputs = deep_model(batch_features)\n",
    "        batch_loss = loss_function(batch_outputs, batch_labels)\n",
    "        batch_loss.backward()\n",
    "        optimizer_unique.step()\n",
    "\n",
    "        accumulated_loss += batch_loss.item() * batch_features.size(0)\n",
    "\n",
    "    avg_epoch_loss = accumulated_loss / len(train_loader_unique.dataset)\n",
    "    print(f'Epoch {epoch_num+1}/{total_epochs}, Loss: {avg_epoch_loss:.4f}')\n",
    "\n",
    "deep_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = deep_model(test_features)\n",
    "    test_predictions = (test_outputs.numpy() > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6307041402236325\n",
      "Precision: 0.4820457018498368\n",
      "Recall: 0.3725820016820858\n",
      "F1 Score: 0.42030360531309297\n"
     ]
    }
   ],
   "source": [
    "#evaluating the model\n",
    "print('Accuracy:', accuracy_score(y_test, test_predictions))\n",
    "print('Precision:', precision_score(y_test, test_predictions))\n",
    "print('Recall:', recall_score(y_test, test_predictions))\n",
    "print('F1 Score:', f1_score(y_test, test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 0.7161\n",
      "Epoch 2/1000, Loss: 0.7091\n",
      "Epoch 3/1000, Loss: 0.7043\n",
      "Epoch 4/1000, Loss: 0.7008\n",
      "Epoch 5/1000, Loss: 0.6984\n",
      "Epoch 6/1000, Loss: 0.6966\n",
      "Epoch 7/1000, Loss: 0.6954\n",
      "Epoch 8/1000, Loss: 0.6946\n",
      "Epoch 9/1000, Loss: 0.6941\n",
      "Epoch 10/1000, Loss: 0.6937\n",
      "Epoch 11/1000, Loss: 0.6935\n",
      "Epoch 12/1000, Loss: 0.6934\n",
      "Epoch 13/1000, Loss: 0.6933\n",
      "Epoch 14/1000, Loss: 0.6932\n",
      "Epoch 15/1000, Loss: 0.6932\n",
      "Epoch 16/1000, Loss: 0.6932\n",
      "Epoch 17/1000, Loss: 0.6932\n",
      "Epoch 18/1000, Loss: 0.6932\n",
      "Epoch 19/1000, Loss: 0.6932\n",
      "Epoch 20/1000, Loss: 0.6932\n",
      "Epoch 21/1000, Loss: 0.6932\n",
      "Epoch 22/1000, Loss: 0.6932\n",
      "Epoch 23/1000, Loss: 0.6932\n",
      "Epoch 24/1000, Loss: 0.6932\n",
      "Epoch 25/1000, Loss: 0.6932\n",
      "Epoch 26/1000, Loss: 0.6932\n",
      "Epoch 27/1000, Loss: 0.6932\n",
      "Epoch 28/1000, Loss: 0.6932\n",
      "Epoch 29/1000, Loss: 0.6932\n",
      "Epoch 30/1000, Loss: 0.6932\n",
      "Epoch 31/1000, Loss: 0.6932\n",
      "Epoch 32/1000, Loss: 0.6932\n",
      "Epoch 33/1000, Loss: 0.6932\n",
      "Epoch 34/1000, Loss: 0.6932\n",
      "Epoch 35/1000, Loss: 0.6932\n",
      "Epoch 36/1000, Loss: 0.6932\n",
      "Epoch 37/1000, Loss: 0.6932\n",
      "Epoch 38/1000, Loss: 0.6932\n",
      "Epoch 39/1000, Loss: 0.6932\n",
      "Epoch 40/1000, Loss: 0.6932\n",
      "Epoch 41/1000, Loss: 0.6932\n",
      "Epoch 42/1000, Loss: 0.6932\n",
      "Epoch 43/1000, Loss: 0.6932\n",
      "Epoch 44/1000, Loss: 0.6932\n",
      "Epoch 45/1000, Loss: 0.6932\n",
      "Epoch 46/1000, Loss: 0.6932\n",
      "Epoch 47/1000, Loss: 0.6932\n",
      "Epoch 48/1000, Loss: 0.6932\n",
      "Epoch 49/1000, Loss: 0.6932\n",
      "Epoch 50/1000, Loss: 0.6932\n",
      "Epoch 51/1000, Loss: 0.6932\n",
      "Epoch 52/1000, Loss: 0.6932\n",
      "Epoch 53/1000, Loss: 0.6932\n",
      "Epoch 54/1000, Loss: 0.6932\n",
      "Epoch 55/1000, Loss: 0.6932\n",
      "Epoch 56/1000, Loss: 0.6932\n",
      "Epoch 57/1000, Loss: 0.6932\n",
      "Epoch 58/1000, Loss: 0.6932\n",
      "Epoch 59/1000, Loss: 0.6932\n",
      "Epoch 60/1000, Loss: 0.6932\n",
      "Epoch 61/1000, Loss: 0.6932\n",
      "Epoch 62/1000, Loss: 0.6932\n",
      "Epoch 63/1000, Loss: 0.6932\n",
      "Epoch 64/1000, Loss: 0.6932\n",
      "Epoch 65/1000, Loss: 0.6932\n",
      "Epoch 66/1000, Loss: 0.6932\n",
      "Epoch 67/1000, Loss: 0.6932\n",
      "Epoch 68/1000, Loss: 0.6932\n",
      "Epoch 69/1000, Loss: 0.6932\n",
      "Epoch 70/1000, Loss: 0.6932\n",
      "Epoch 71/1000, Loss: 0.6932\n",
      "Epoch 72/1000, Loss: 0.6932\n",
      "Epoch 73/1000, Loss: 0.6932\n",
      "Epoch 74/1000, Loss: 0.6932\n",
      "Epoch 75/1000, Loss: 0.6932\n",
      "Epoch 76/1000, Loss: 0.6932\n",
      "Epoch 77/1000, Loss: 0.6932\n",
      "Epoch 78/1000, Loss: 0.6932\n",
      "Epoch 79/1000, Loss: 0.6932\n",
      "Epoch 80/1000, Loss: 0.6931\n",
      "Epoch 81/1000, Loss: 0.6932\n",
      "Epoch 82/1000, Loss: 0.6932\n",
      "Epoch 83/1000, Loss: 0.6932\n",
      "Epoch 84/1000, Loss: 0.6932\n",
      "Epoch 85/1000, Loss: 0.6932\n",
      "Epoch 86/1000, Loss: 0.6932\n",
      "Epoch 87/1000, Loss: 0.6932\n",
      "Epoch 88/1000, Loss: 0.6932\n",
      "Epoch 89/1000, Loss: 0.6932\n",
      "Epoch 90/1000, Loss: 0.6932\n",
      "Epoch 91/1000, Loss: 0.6932\n",
      "Epoch 92/1000, Loss: 0.6932\n",
      "Epoch 93/1000, Loss: 0.6932\n",
      "Epoch 94/1000, Loss: 0.6932\n",
      "Epoch 95/1000, Loss: 0.6932\n",
      "Epoch 96/1000, Loss: 0.6932\n",
      "Epoch 97/1000, Loss: 0.6932\n",
      "Epoch 98/1000, Loss: 0.6932\n",
      "Epoch 99/1000, Loss: 0.6932\n",
      "Epoch 100/1000, Loss: 0.6932\n",
      "Epoch 101/1000, Loss: 0.6932\n",
      "Epoch 102/1000, Loss: 0.6932\n",
      "Epoch 103/1000, Loss: 0.6932\n",
      "Epoch 104/1000, Loss: 0.6932\n",
      "Epoch 105/1000, Loss: 0.6932\n",
      "Epoch 106/1000, Loss: 0.6932\n",
      "Epoch 107/1000, Loss: 0.6932\n",
      "Epoch 108/1000, Loss: 0.6932\n",
      "Epoch 109/1000, Loss: 0.6932\n",
      "Epoch 110/1000, Loss: 0.6932\n",
      "Epoch 111/1000, Loss: 0.6932\n",
      "Epoch 112/1000, Loss: 0.6932\n",
      "Epoch 113/1000, Loss: 0.6932\n",
      "Epoch 114/1000, Loss: 0.6932\n",
      "Epoch 115/1000, Loss: 0.6932\n",
      "Epoch 116/1000, Loss: 0.6932\n",
      "Epoch 117/1000, Loss: 0.6932\n",
      "Epoch 118/1000, Loss: 0.6932\n",
      "Epoch 119/1000, Loss: 0.6932\n",
      "Epoch 120/1000, Loss: 0.6932\n",
      "Epoch 121/1000, Loss: 0.6932\n",
      "Epoch 122/1000, Loss: 0.6932\n",
      "Epoch 123/1000, Loss: 0.6932\n",
      "Epoch 124/1000, Loss: 0.6932\n",
      "Epoch 125/1000, Loss: 0.6932\n",
      "Epoch 126/1000, Loss: 0.6932\n",
      "Epoch 127/1000, Loss: 0.6932\n",
      "Epoch 128/1000, Loss: 0.6932\n",
      "Epoch 129/1000, Loss: 0.6932\n",
      "Epoch 130/1000, Loss: 0.6932\n",
      "Epoch 131/1000, Loss: 0.6932\n",
      "Epoch 132/1000, Loss: 0.6932\n",
      "Epoch 133/1000, Loss: 0.6932\n",
      "Epoch 134/1000, Loss: 0.6932\n",
      "Epoch 135/1000, Loss: 0.6932\n",
      "Epoch 136/1000, Loss: 0.6932\n",
      "Epoch 137/1000, Loss: 0.6932\n",
      "Epoch 138/1000, Loss: 0.6932\n",
      "Epoch 139/1000, Loss: 0.6932\n",
      "Epoch 140/1000, Loss: 0.6932\n",
      "Epoch 141/1000, Loss: 0.6932\n",
      "Epoch 142/1000, Loss: 0.6932\n",
      "Epoch 143/1000, Loss: 0.6932\n",
      "Epoch 144/1000, Loss: 0.6932\n",
      "Epoch 145/1000, Loss: 0.6932\n",
      "Epoch 146/1000, Loss: 0.6932\n",
      "Epoch 147/1000, Loss: 0.6932\n",
      "Epoch 148/1000, Loss: 0.6932\n",
      "Epoch 149/1000, Loss: 0.6932\n",
      "Epoch 150/1000, Loss: 0.6932\n",
      "Epoch 151/1000, Loss: 0.6932\n",
      "Epoch 152/1000, Loss: 0.6932\n",
      "Epoch 153/1000, Loss: 0.6932\n",
      "Epoch 154/1000, Loss: 0.6932\n",
      "Epoch 155/1000, Loss: 0.6932\n",
      "Epoch 156/1000, Loss: 0.6932\n",
      "Epoch 157/1000, Loss: 0.6932\n",
      "Epoch 158/1000, Loss: 0.6932\n",
      "Epoch 159/1000, Loss: 0.6932\n",
      "Epoch 160/1000, Loss: 0.6932\n",
      "Epoch 161/1000, Loss: 0.6932\n",
      "Epoch 162/1000, Loss: 0.6932\n",
      "Epoch 163/1000, Loss: 0.6932\n",
      "Epoch 164/1000, Loss: 0.6932\n",
      "Epoch 165/1000, Loss: 0.6932\n",
      "Epoch 166/1000, Loss: 0.6932\n",
      "Epoch 167/1000, Loss: 0.6932\n",
      "Epoch 168/1000, Loss: 0.6932\n",
      "Epoch 169/1000, Loss: 0.6932\n",
      "Epoch 170/1000, Loss: 0.6932\n",
      "Epoch 171/1000, Loss: 0.6932\n",
      "Epoch 172/1000, Loss: 0.6932\n",
      "Epoch 173/1000, Loss: 0.6932\n",
      "Epoch 174/1000, Loss: 0.6932\n",
      "Epoch 175/1000, Loss: 0.6932\n",
      "Epoch 176/1000, Loss: 0.6932\n",
      "Epoch 177/1000, Loss: 0.6932\n",
      "Epoch 178/1000, Loss: 0.6932\n",
      "Epoch 179/1000, Loss: 0.6932\n",
      "Epoch 180/1000, Loss: 0.6932\n",
      "Epoch 181/1000, Loss: 0.6932\n",
      "Epoch 182/1000, Loss: 0.6932\n",
      "Epoch 183/1000, Loss: 0.6932\n",
      "Epoch 184/1000, Loss: 0.6932\n",
      "Epoch 185/1000, Loss: 0.6932\n",
      "Epoch 186/1000, Loss: 0.6932\n",
      "Epoch 187/1000, Loss: 0.6932\n",
      "Epoch 188/1000, Loss: 0.6932\n",
      "Epoch 189/1000, Loss: 0.6932\n",
      "Epoch 190/1000, Loss: 0.6932\n",
      "Epoch 191/1000, Loss: 0.6932\n",
      "Epoch 192/1000, Loss: 0.6932\n",
      "Epoch 193/1000, Loss: 0.6932\n",
      "Epoch 194/1000, Loss: 0.6932\n",
      "Epoch 195/1000, Loss: 0.6932\n",
      "Epoch 196/1000, Loss: 0.6932\n",
      "Epoch 197/1000, Loss: 0.6932\n",
      "Epoch 198/1000, Loss: 0.6932\n",
      "Epoch 199/1000, Loss: 0.6932\n",
      "Epoch 200/1000, Loss: 0.6932\n",
      "Epoch 201/1000, Loss: 0.6932\n",
      "Epoch 202/1000, Loss: 0.6932\n",
      "Epoch 203/1000, Loss: 0.6932\n",
      "Epoch 204/1000, Loss: 0.6932\n",
      "Epoch 205/1000, Loss: 0.6932\n",
      "Epoch 206/1000, Loss: 0.6932\n",
      "Epoch 207/1000, Loss: 0.6932\n",
      "Epoch 208/1000, Loss: 0.6932\n",
      "Epoch 209/1000, Loss: 0.6932\n",
      "Epoch 210/1000, Loss: 0.6932\n",
      "Epoch 211/1000, Loss: 0.6932\n",
      "Epoch 212/1000, Loss: 0.6932\n",
      "Epoch 213/1000, Loss: 0.6932\n",
      "Epoch 214/1000, Loss: 0.6932\n",
      "Epoch 215/1000, Loss: 0.6932\n",
      "Epoch 216/1000, Loss: 0.6932\n",
      "Epoch 217/1000, Loss: 0.6932\n",
      "Epoch 218/1000, Loss: 0.6932\n",
      "Epoch 219/1000, Loss: 0.6932\n",
      "Epoch 220/1000, Loss: 0.6932\n",
      "Epoch 221/1000, Loss: 0.6932\n",
      "Epoch 222/1000, Loss: 0.6932\n",
      "Epoch 223/1000, Loss: 0.6932\n",
      "Epoch 224/1000, Loss: 0.6932\n",
      "Epoch 225/1000, Loss: 0.6932\n",
      "Epoch 226/1000, Loss: 0.6932\n",
      "Epoch 227/1000, Loss: 0.6932\n",
      "Epoch 228/1000, Loss: 0.6932\n",
      "Epoch 229/1000, Loss: 0.6932\n",
      "Epoch 230/1000, Loss: 0.6932\n",
      "Epoch 231/1000, Loss: 0.6932\n",
      "Epoch 232/1000, Loss: 0.6932\n",
      "Epoch 233/1000, Loss: 0.6932\n",
      "Epoch 234/1000, Loss: 0.6932\n",
      "Epoch 235/1000, Loss: 0.6932\n",
      "Epoch 236/1000, Loss: 0.6932\n",
      "Epoch 237/1000, Loss: 0.6932\n",
      "Epoch 238/1000, Loss: 0.6932\n",
      "Epoch 239/1000, Loss: 0.6932\n",
      "Epoch 240/1000, Loss: 0.6932\n",
      "Epoch 241/1000, Loss: 0.6932\n",
      "Epoch 242/1000, Loss: 0.6932\n",
      "Epoch 243/1000, Loss: 0.6932\n",
      "Epoch 244/1000, Loss: 0.6932\n",
      "Epoch 245/1000, Loss: 0.6932\n",
      "Epoch 246/1000, Loss: 0.6932\n",
      "Epoch 247/1000, Loss: 0.6932\n",
      "Epoch 248/1000, Loss: 0.6932\n",
      "Epoch 249/1000, Loss: 0.6932\n",
      "Epoch 250/1000, Loss: 0.6932\n",
      "Epoch 251/1000, Loss: 0.6932\n",
      "Epoch 252/1000, Loss: 0.6932\n",
      "Epoch 253/1000, Loss: 0.6932\n",
      "Epoch 254/1000, Loss: 0.6932\n",
      "Epoch 255/1000, Loss: 0.6932\n",
      "Epoch 256/1000, Loss: 0.6932\n",
      "Epoch 257/1000, Loss: 0.6932\n",
      "Epoch 258/1000, Loss: 0.6932\n",
      "Epoch 259/1000, Loss: 0.6932\n",
      "Epoch 260/1000, Loss: 0.6932\n",
      "Epoch 261/1000, Loss: 0.6932\n",
      "Epoch 262/1000, Loss: 0.6932\n",
      "Epoch 263/1000, Loss: 0.6932\n",
      "Epoch 264/1000, Loss: 0.6932\n",
      "Epoch 265/1000, Loss: 0.6932\n",
      "Epoch 266/1000, Loss: 0.6932\n",
      "Epoch 267/1000, Loss: 0.6932\n",
      "Epoch 268/1000, Loss: 0.6932\n",
      "Epoch 269/1000, Loss: 0.6932\n",
      "Epoch 270/1000, Loss: 0.6932\n",
      "Epoch 271/1000, Loss: 0.6932\n",
      "Epoch 272/1000, Loss: 0.6932\n",
      "Epoch 273/1000, Loss: 0.6932\n",
      "Epoch 274/1000, Loss: 0.6932\n",
      "Epoch 275/1000, Loss: 0.6932\n",
      "Epoch 276/1000, Loss: 0.6932\n",
      "Epoch 277/1000, Loss: 0.6932\n",
      "Epoch 278/1000, Loss: 0.6932\n",
      "Epoch 279/1000, Loss: 0.6932\n",
      "Epoch 280/1000, Loss: 0.6932\n",
      "Epoch 281/1000, Loss: 0.6932\n",
      "Epoch 282/1000, Loss: 0.6932\n",
      "Epoch 283/1000, Loss: 0.6932\n",
      "Epoch 284/1000, Loss: 0.6932\n",
      "Epoch 285/1000, Loss: 0.6932\n",
      "Epoch 286/1000, Loss: 0.6932\n",
      "Epoch 287/1000, Loss: 0.6932\n",
      "Epoch 288/1000, Loss: 0.6932\n",
      "Epoch 289/1000, Loss: 0.6932\n",
      "Epoch 290/1000, Loss: 0.6932\n",
      "Epoch 291/1000, Loss: 0.6932\n",
      "Epoch 292/1000, Loss: 0.6932\n",
      "Epoch 293/1000, Loss: 0.6932\n",
      "Epoch 294/1000, Loss: 0.6932\n",
      "Epoch 295/1000, Loss: 0.6932\n",
      "Epoch 296/1000, Loss: 0.6932\n",
      "Epoch 297/1000, Loss: 0.6932\n",
      "Epoch 298/1000, Loss: 0.6932\n",
      "Epoch 299/1000, Loss: 0.6932\n",
      "Epoch 300/1000, Loss: 0.6932\n",
      "Epoch 301/1000, Loss: 0.6932\n",
      "Epoch 302/1000, Loss: 0.6932\n",
      "Epoch 303/1000, Loss: 0.6932\n",
      "Epoch 304/1000, Loss: 0.6932\n",
      "Epoch 305/1000, Loss: 0.6932\n",
      "Epoch 306/1000, Loss: 0.6932\n",
      "Epoch 307/1000, Loss: 0.6932\n",
      "Epoch 308/1000, Loss: 0.6932\n",
      "Epoch 309/1000, Loss: 0.6932\n",
      "Epoch 310/1000, Loss: 0.6932\n",
      "Epoch 311/1000, Loss: 0.6932\n",
      "Epoch 312/1000, Loss: 0.6932\n",
      "Epoch 313/1000, Loss: 0.6932\n",
      "Epoch 314/1000, Loss: 0.6932\n",
      "Epoch 315/1000, Loss: 0.6932\n",
      "Epoch 316/1000, Loss: 0.6932\n",
      "Epoch 317/1000, Loss: 0.6932\n",
      "Epoch 318/1000, Loss: 0.6932\n",
      "Epoch 319/1000, Loss: 0.6932\n",
      "Epoch 320/1000, Loss: 0.6932\n",
      "Epoch 321/1000, Loss: 0.6932\n",
      "Epoch 322/1000, Loss: 0.6932\n",
      "Epoch 323/1000, Loss: 0.6932\n",
      "Epoch 324/1000, Loss: 0.6932\n",
      "Epoch 325/1000, Loss: 0.6932\n",
      "Epoch 326/1000, Loss: 0.6932\n",
      "Epoch 327/1000, Loss: 0.6932\n",
      "Epoch 328/1000, Loss: 0.6932\n",
      "Epoch 329/1000, Loss: 0.6932\n",
      "Epoch 330/1000, Loss: 0.6932\n",
      "Epoch 331/1000, Loss: 0.6932\n",
      "Epoch 332/1000, Loss: 0.6932\n",
      "Epoch 333/1000, Loss: 0.6932\n",
      "Epoch 334/1000, Loss: 0.6932\n",
      "Epoch 335/1000, Loss: 0.6932\n",
      "Epoch 336/1000, Loss: 0.6932\n",
      "Epoch 337/1000, Loss: 0.6932\n",
      "Epoch 338/1000, Loss: 0.6932\n",
      "Epoch 339/1000, Loss: 0.6932\n",
      "Epoch 340/1000, Loss: 0.6932\n",
      "Epoch 341/1000, Loss: 0.6932\n",
      "Epoch 342/1000, Loss: 0.6932\n",
      "Epoch 343/1000, Loss: 0.6932\n",
      "Epoch 344/1000, Loss: 0.6932\n",
      "Epoch 345/1000, Loss: 0.6932\n",
      "Epoch 346/1000, Loss: 0.6932\n",
      "Epoch 347/1000, Loss: 0.6932\n",
      "Epoch 348/1000, Loss: 0.6932\n",
      "Epoch 349/1000, Loss: 0.6932\n",
      "Epoch 350/1000, Loss: 0.6932\n",
      "Epoch 351/1000, Loss: 0.6932\n",
      "Epoch 352/1000, Loss: 0.6932\n",
      "Epoch 353/1000, Loss: 0.6932\n",
      "Epoch 354/1000, Loss: 0.6932\n",
      "Epoch 355/1000, Loss: 0.6932\n",
      "Epoch 356/1000, Loss: 0.6932\n",
      "Epoch 357/1000, Loss: 0.6932\n",
      "Epoch 358/1000, Loss: 0.6932\n",
      "Epoch 359/1000, Loss: 0.6932\n",
      "Epoch 360/1000, Loss: 0.6932\n",
      "Epoch 361/1000, Loss: 0.6932\n",
      "Epoch 362/1000, Loss: 0.6932\n",
      "Epoch 363/1000, Loss: 0.6932\n",
      "Epoch 364/1000, Loss: 0.6932\n",
      "Epoch 365/1000, Loss: 0.6932\n",
      "Epoch 366/1000, Loss: 0.6932\n",
      "Epoch 367/1000, Loss: 0.6932\n",
      "Epoch 368/1000, Loss: 0.6932\n",
      "Epoch 369/1000, Loss: 0.6932\n",
      "Epoch 370/1000, Loss: 0.6932\n",
      "Epoch 371/1000, Loss: 0.6932\n",
      "Epoch 372/1000, Loss: 0.6932\n",
      "Epoch 373/1000, Loss: 0.6932\n",
      "Epoch 374/1000, Loss: 0.6932\n",
      "Epoch 375/1000, Loss: 0.6932\n",
      "Epoch 376/1000, Loss: 0.6932\n",
      "Epoch 377/1000, Loss: 0.6932\n",
      "Epoch 378/1000, Loss: 0.6932\n",
      "Epoch 379/1000, Loss: 0.6932\n",
      "Epoch 380/1000, Loss: 0.6932\n",
      "Epoch 381/1000, Loss: 0.6932\n",
      "Epoch 382/1000, Loss: 0.6932\n",
      "Epoch 383/1000, Loss: 0.6932\n",
      "Epoch 384/1000, Loss: 0.6932\n",
      "Epoch 385/1000, Loss: 0.6932\n",
      "Epoch 386/1000, Loss: 0.6932\n",
      "Epoch 387/1000, Loss: 0.6932\n",
      "Epoch 388/1000, Loss: 0.6932\n",
      "Epoch 389/1000, Loss: 0.6932\n",
      "Epoch 390/1000, Loss: 0.6932\n",
      "Epoch 391/1000, Loss: 0.6932\n",
      "Epoch 392/1000, Loss: 0.6932\n",
      "Epoch 393/1000, Loss: 0.6932\n",
      "Epoch 394/1000, Loss: 0.6932\n",
      "Epoch 395/1000, Loss: 0.6932\n",
      "Epoch 396/1000, Loss: 0.6932\n",
      "Epoch 397/1000, Loss: 0.6932\n",
      "Epoch 398/1000, Loss: 0.6932\n",
      "Epoch 399/1000, Loss: 0.6932\n",
      "Epoch 400/1000, Loss: 0.6932\n",
      "Epoch 401/1000, Loss: 0.6932\n",
      "Epoch 402/1000, Loss: 0.6932\n",
      "Epoch 403/1000, Loss: 0.6932\n",
      "Epoch 404/1000, Loss: 0.6932\n",
      "Epoch 405/1000, Loss: 0.6932\n",
      "Epoch 406/1000, Loss: 0.6932\n",
      "Epoch 407/1000, Loss: 0.6932\n",
      "Epoch 408/1000, Loss: 0.6932\n",
      "Epoch 409/1000, Loss: 0.6932\n",
      "Epoch 410/1000, Loss: 0.6932\n",
      "Epoch 411/1000, Loss: 0.6932\n",
      "Epoch 412/1000, Loss: 0.6932\n",
      "Epoch 413/1000, Loss: 0.6932\n",
      "Epoch 414/1000, Loss: 0.6932\n",
      "Epoch 415/1000, Loss: 0.6932\n",
      "Epoch 416/1000, Loss: 0.6932\n",
      "Epoch 417/1000, Loss: 0.6932\n",
      "Epoch 418/1000, Loss: 0.6932\n",
      "Epoch 419/1000, Loss: 0.6932\n",
      "Epoch 420/1000, Loss: 0.6932\n",
      "Epoch 421/1000, Loss: 0.6932\n",
      "Epoch 422/1000, Loss: 0.6932\n",
      "Epoch 423/1000, Loss: 0.6932\n",
      "Epoch 424/1000, Loss: 0.6932\n",
      "Epoch 425/1000, Loss: 0.6932\n",
      "Epoch 426/1000, Loss: 0.6932\n",
      "Epoch 427/1000, Loss: 0.6932\n",
      "Epoch 428/1000, Loss: 0.6932\n",
      "Epoch 429/1000, Loss: 0.6932\n",
      "Epoch 430/1000, Loss: 0.6932\n",
      "Epoch 431/1000, Loss: 0.6932\n",
      "Epoch 432/1000, Loss: 0.6932\n",
      "Epoch 433/1000, Loss: 0.6932\n",
      "Epoch 434/1000, Loss: 0.6932\n",
      "Epoch 435/1000, Loss: 0.6932\n",
      "Epoch 436/1000, Loss: 0.6932\n",
      "Epoch 437/1000, Loss: 0.6932\n",
      "Epoch 438/1000, Loss: 0.6932\n",
      "Epoch 439/1000, Loss: 0.6932\n",
      "Epoch 440/1000, Loss: 0.6932\n",
      "Epoch 441/1000, Loss: 0.6932\n",
      "Epoch 442/1000, Loss: 0.6932\n",
      "Epoch 443/1000, Loss: 0.6932\n",
      "Epoch 444/1000, Loss: 0.6932\n",
      "Epoch 445/1000, Loss: 0.6932\n",
      "Epoch 446/1000, Loss: 0.6932\n",
      "Epoch 447/1000, Loss: 0.6932\n",
      "Epoch 448/1000, Loss: 0.6932\n",
      "Epoch 449/1000, Loss: 0.6932\n",
      "Epoch 450/1000, Loss: 0.6932\n",
      "Epoch 451/1000, Loss: 0.6932\n",
      "Epoch 452/1000, Loss: 0.6932\n",
      "Epoch 453/1000, Loss: 0.6932\n",
      "Epoch 454/1000, Loss: 0.6932\n",
      "Epoch 455/1000, Loss: 0.6932\n",
      "Epoch 456/1000, Loss: 0.6932\n",
      "Epoch 457/1000, Loss: 0.6932\n",
      "Epoch 458/1000, Loss: 0.6932\n",
      "Epoch 459/1000, Loss: 0.6932\n",
      "Epoch 460/1000, Loss: 0.6932\n",
      "Epoch 461/1000, Loss: 0.6932\n",
      "Epoch 462/1000, Loss: 0.6932\n",
      "Epoch 463/1000, Loss: 0.6932\n",
      "Epoch 464/1000, Loss: 0.6932\n",
      "Epoch 465/1000, Loss: 0.6932\n",
      "Epoch 466/1000, Loss: 0.6932\n",
      "Epoch 467/1000, Loss: 0.6932\n",
      "Epoch 468/1000, Loss: 0.6932\n",
      "Epoch 469/1000, Loss: 0.6932\n",
      "Epoch 470/1000, Loss: 0.6932\n",
      "Epoch 471/1000, Loss: 0.6932\n",
      "Epoch 472/1000, Loss: 0.6932\n",
      "Epoch 473/1000, Loss: 0.6932\n",
      "Epoch 474/1000, Loss: 0.6932\n",
      "Epoch 475/1000, Loss: 0.6932\n",
      "Epoch 476/1000, Loss: 0.6932\n",
      "Epoch 477/1000, Loss: 0.6932\n",
      "Epoch 478/1000, Loss: 0.6932\n",
      "Epoch 479/1000, Loss: 0.6932\n",
      "Epoch 480/1000, Loss: 0.6932\n",
      "Epoch 481/1000, Loss: 0.6932\n",
      "Epoch 482/1000, Loss: 0.6932\n",
      "Epoch 483/1000, Loss: 0.6932\n",
      "Epoch 484/1000, Loss: 0.6932\n",
      "Epoch 485/1000, Loss: 0.6932\n",
      "Epoch 486/1000, Loss: 0.6932\n",
      "Epoch 487/1000, Loss: 0.6932\n",
      "Epoch 488/1000, Loss: 0.6932\n",
      "Epoch 489/1000, Loss: 0.6932\n",
      "Epoch 490/1000, Loss: 0.6932\n",
      "Epoch 491/1000, Loss: 0.6932\n",
      "Epoch 492/1000, Loss: 0.6932\n",
      "Epoch 493/1000, Loss: 0.6932\n",
      "Epoch 494/1000, Loss: 0.6932\n",
      "Epoch 495/1000, Loss: 0.6932\n",
      "Epoch 496/1000, Loss: 0.6932\n",
      "Epoch 497/1000, Loss: 0.6932\n",
      "Epoch 498/1000, Loss: 0.6932\n",
      "Epoch 499/1000, Loss: 0.6932\n",
      "Epoch 500/1000, Loss: 0.6932\n",
      "Epoch 501/1000, Loss: 0.6932\n",
      "Epoch 502/1000, Loss: 0.6932\n",
      "Epoch 503/1000, Loss: 0.6932\n",
      "Epoch 504/1000, Loss: 0.6932\n",
      "Epoch 505/1000, Loss: 0.6932\n",
      "Epoch 506/1000, Loss: 0.6932\n",
      "Epoch 507/1000, Loss: 0.6932\n",
      "Epoch 508/1000, Loss: 0.6932\n",
      "Epoch 509/1000, Loss: 0.6932\n",
      "Epoch 510/1000, Loss: 0.6932\n",
      "Epoch 511/1000, Loss: 0.6932\n",
      "Epoch 512/1000, Loss: 0.6932\n",
      "Epoch 513/1000, Loss: 0.6932\n",
      "Epoch 514/1000, Loss: 0.6932\n",
      "Epoch 515/1000, Loss: 0.6932\n",
      "Epoch 516/1000, Loss: 0.6932\n",
      "Epoch 517/1000, Loss: 0.6932\n",
      "Epoch 518/1000, Loss: 0.6932\n",
      "Epoch 519/1000, Loss: 0.6932\n",
      "Epoch 520/1000, Loss: 0.6932\n",
      "Epoch 521/1000, Loss: 0.6932\n",
      "Epoch 522/1000, Loss: 0.6932\n",
      "Epoch 523/1000, Loss: 0.6932\n",
      "Epoch 524/1000, Loss: 0.6932\n",
      "Epoch 525/1000, Loss: 0.6932\n",
      "Epoch 526/1000, Loss: 0.6932\n",
      "Epoch 527/1000, Loss: 0.6932\n",
      "Epoch 528/1000, Loss: 0.6932\n",
      "Epoch 529/1000, Loss: 0.6932\n",
      "Epoch 530/1000, Loss: 0.6932\n",
      "Epoch 531/1000, Loss: 0.6932\n",
      "Epoch 532/1000, Loss: 0.6932\n",
      "Epoch 533/1000, Loss: 0.6932\n",
      "Epoch 534/1000, Loss: 0.6932\n",
      "Epoch 535/1000, Loss: 0.6932\n",
      "Epoch 536/1000, Loss: 0.6932\n",
      "Epoch 537/1000, Loss: 0.6932\n",
      "Epoch 538/1000, Loss: 0.6932\n",
      "Epoch 539/1000, Loss: 0.6932\n",
      "Epoch 540/1000, Loss: 0.6932\n",
      "Epoch 541/1000, Loss: 0.6932\n",
      "Epoch 542/1000, Loss: 0.6932\n",
      "Epoch 543/1000, Loss: 0.6932\n",
      "Epoch 544/1000, Loss: 0.6932\n",
      "Epoch 545/1000, Loss: 0.6932\n",
      "Epoch 546/1000, Loss: 0.6932\n",
      "Epoch 547/1000, Loss: 0.6932\n",
      "Epoch 548/1000, Loss: 0.6932\n",
      "Epoch 549/1000, Loss: 0.6932\n",
      "Epoch 550/1000, Loss: 0.6932\n",
      "Epoch 551/1000, Loss: 0.6932\n",
      "Epoch 552/1000, Loss: 0.6932\n",
      "Epoch 553/1000, Loss: 0.6932\n",
      "Epoch 554/1000, Loss: 0.6932\n",
      "Epoch 555/1000, Loss: 0.6932\n",
      "Epoch 556/1000, Loss: 0.6932\n",
      "Epoch 557/1000, Loss: 0.6932\n",
      "Epoch 558/1000, Loss: 0.6932\n",
      "Epoch 559/1000, Loss: 0.6932\n",
      "Epoch 560/1000, Loss: 0.6932\n",
      "Epoch 561/1000, Loss: 0.6932\n",
      "Epoch 562/1000, Loss: 0.6932\n",
      "Epoch 563/1000, Loss: 0.6932\n",
      "Epoch 564/1000, Loss: 0.6932\n",
      "Epoch 565/1000, Loss: 0.6932\n",
      "Epoch 566/1000, Loss: 0.6932\n",
      "Epoch 567/1000, Loss: 0.6932\n",
      "Epoch 568/1000, Loss: 0.6932\n",
      "Epoch 569/1000, Loss: 0.6932\n",
      "Epoch 570/1000, Loss: 0.6932\n",
      "Epoch 571/1000, Loss: 0.6932\n",
      "Epoch 572/1000, Loss: 0.6932\n",
      "Epoch 573/1000, Loss: 0.6932\n",
      "Epoch 574/1000, Loss: 0.6932\n",
      "Epoch 575/1000, Loss: 0.6932\n",
      "Epoch 576/1000, Loss: 0.6932\n",
      "Epoch 577/1000, Loss: 0.6932\n",
      "Epoch 578/1000, Loss: 0.6932\n",
      "Epoch 579/1000, Loss: 0.6932\n",
      "Epoch 580/1000, Loss: 0.6932\n",
      "Epoch 581/1000, Loss: 0.6932\n",
      "Epoch 582/1000, Loss: 0.6932\n",
      "Epoch 583/1000, Loss: 0.6932\n",
      "Epoch 584/1000, Loss: 0.6932\n",
      "Epoch 585/1000, Loss: 0.6932\n",
      "Epoch 586/1000, Loss: 0.6932\n",
      "Epoch 587/1000, Loss: 0.6932\n",
      "Epoch 588/1000, Loss: 0.6932\n",
      "Epoch 589/1000, Loss: 0.6932\n",
      "Epoch 590/1000, Loss: 0.6932\n",
      "Epoch 591/1000, Loss: 0.6932\n",
      "Epoch 592/1000, Loss: 0.6932\n",
      "Epoch 593/1000, Loss: 0.6932\n",
      "Epoch 594/1000, Loss: 0.6932\n",
      "Epoch 595/1000, Loss: 0.6932\n",
      "Epoch 596/1000, Loss: 0.6932\n",
      "Epoch 597/1000, Loss: 0.6932\n",
      "Epoch 598/1000, Loss: 0.6932\n",
      "Epoch 599/1000, Loss: 0.6932\n",
      "Epoch 600/1000, Loss: 0.6932\n",
      "Epoch 601/1000, Loss: 0.6932\n",
      "Epoch 602/1000, Loss: 0.6932\n",
      "Epoch 603/1000, Loss: 0.6932\n",
      "Epoch 604/1000, Loss: 0.6932\n",
      "Epoch 605/1000, Loss: 0.6932\n",
      "Epoch 606/1000, Loss: 0.6932\n",
      "Epoch 607/1000, Loss: 0.6932\n",
      "Epoch 608/1000, Loss: 0.6932\n",
      "Epoch 609/1000, Loss: 0.6932\n",
      "Epoch 610/1000, Loss: 0.6932\n",
      "Epoch 611/1000, Loss: 0.6932\n",
      "Epoch 612/1000, Loss: 0.6932\n",
      "Epoch 613/1000, Loss: 0.6932\n",
      "Epoch 614/1000, Loss: 0.6932\n",
      "Epoch 615/1000, Loss: 0.6932\n",
      "Epoch 616/1000, Loss: 0.6932\n",
      "Epoch 617/1000, Loss: 0.6932\n",
      "Epoch 618/1000, Loss: 0.6932\n",
      "Epoch 619/1000, Loss: 0.6932\n",
      "Epoch 620/1000, Loss: 0.6932\n",
      "Epoch 621/1000, Loss: 0.6932\n",
      "Epoch 622/1000, Loss: 0.6932\n",
      "Epoch 623/1000, Loss: 0.6932\n",
      "Epoch 624/1000, Loss: 0.6932\n",
      "Epoch 625/1000, Loss: 0.6932\n",
      "Epoch 626/1000, Loss: 0.6932\n",
      "Epoch 627/1000, Loss: 0.6932\n",
      "Epoch 628/1000, Loss: 0.6932\n",
      "Epoch 629/1000, Loss: 0.6932\n",
      "Epoch 630/1000, Loss: 0.6932\n",
      "Epoch 631/1000, Loss: 0.6932\n",
      "Epoch 632/1000, Loss: 0.6932\n",
      "Epoch 633/1000, Loss: 0.6932\n",
      "Epoch 634/1000, Loss: 0.6932\n",
      "Epoch 635/1000, Loss: 0.6932\n",
      "Epoch 636/1000, Loss: 0.6932\n",
      "Epoch 637/1000, Loss: 0.6932\n",
      "Epoch 638/1000, Loss: 0.6932\n",
      "Epoch 639/1000, Loss: 0.6932\n",
      "Epoch 640/1000, Loss: 0.6932\n",
      "Epoch 641/1000, Loss: 0.6932\n",
      "Epoch 642/1000, Loss: 0.6932\n",
      "Epoch 643/1000, Loss: 0.6932\n",
      "Epoch 644/1000, Loss: 0.6932\n",
      "Epoch 645/1000, Loss: 0.6932\n",
      "Epoch 646/1000, Loss: 0.6932\n",
      "Epoch 647/1000, Loss: 0.6932\n",
      "Epoch 648/1000, Loss: 0.6932\n",
      "Epoch 649/1000, Loss: 0.6932\n",
      "Epoch 650/1000, Loss: 0.6932\n",
      "Epoch 651/1000, Loss: 0.6932\n",
      "Epoch 652/1000, Loss: 0.6932\n",
      "Epoch 653/1000, Loss: 0.6932\n",
      "Epoch 654/1000, Loss: 0.6932\n",
      "Epoch 655/1000, Loss: 0.6932\n",
      "Epoch 656/1000, Loss: 0.6932\n",
      "Epoch 657/1000, Loss: 0.6932\n",
      "Epoch 658/1000, Loss: 0.6932\n",
      "Epoch 659/1000, Loss: 0.6932\n",
      "Epoch 660/1000, Loss: 0.6932\n",
      "Epoch 661/1000, Loss: 0.6932\n",
      "Epoch 662/1000, Loss: 0.6932\n",
      "Epoch 663/1000, Loss: 0.6932\n",
      "Epoch 664/1000, Loss: 0.6932\n",
      "Epoch 665/1000, Loss: 0.6932\n",
      "Epoch 666/1000, Loss: 0.6932\n",
      "Epoch 667/1000, Loss: 0.6932\n",
      "Epoch 668/1000, Loss: 0.6932\n",
      "Epoch 669/1000, Loss: 0.6932\n",
      "Epoch 670/1000, Loss: 0.6932\n",
      "Epoch 671/1000, Loss: 0.6932\n",
      "Epoch 672/1000, Loss: 0.6932\n",
      "Epoch 673/1000, Loss: 0.6932\n",
      "Epoch 674/1000, Loss: 0.6932\n",
      "Epoch 675/1000, Loss: 0.6932\n",
      "Epoch 676/1000, Loss: 0.6932\n",
      "Epoch 677/1000, Loss: 0.6932\n",
      "Epoch 678/1000, Loss: 0.6932\n",
      "Epoch 679/1000, Loss: 0.6932\n",
      "Epoch 680/1000, Loss: 0.6932\n",
      "Epoch 681/1000, Loss: 0.6932\n",
      "Epoch 682/1000, Loss: 0.6932\n",
      "Epoch 683/1000, Loss: 0.6932\n",
      "Epoch 684/1000, Loss: 0.6932\n",
      "Epoch 685/1000, Loss: 0.6932\n",
      "Epoch 686/1000, Loss: 0.6932\n",
      "Epoch 687/1000, Loss: 0.6932\n",
      "Epoch 688/1000, Loss: 0.6932\n",
      "Epoch 689/1000, Loss: 0.6932\n",
      "Epoch 690/1000, Loss: 0.6932\n",
      "Epoch 691/1000, Loss: 0.6932\n",
      "Epoch 692/1000, Loss: 0.6932\n",
      "Epoch 693/1000, Loss: 0.6932\n",
      "Epoch 694/1000, Loss: 0.6932\n",
      "Epoch 695/1000, Loss: 0.6932\n",
      "Epoch 696/1000, Loss: 0.6932\n",
      "Epoch 697/1000, Loss: 0.6932\n",
      "Epoch 698/1000, Loss: 0.6932\n",
      "Epoch 699/1000, Loss: 0.6932\n",
      "Epoch 700/1000, Loss: 0.6932\n",
      "Epoch 701/1000, Loss: 0.6932\n",
      "Epoch 702/1000, Loss: 0.6932\n",
      "Epoch 703/1000, Loss: 0.6932\n",
      "Epoch 704/1000, Loss: 0.6932\n",
      "Epoch 705/1000, Loss: 0.6932\n",
      "Epoch 706/1000, Loss: 0.6932\n",
      "Epoch 707/1000, Loss: 0.6932\n",
      "Epoch 708/1000, Loss: 0.6932\n",
      "Epoch 709/1000, Loss: 0.6932\n",
      "Epoch 710/1000, Loss: 0.6932\n",
      "Epoch 711/1000, Loss: 0.6932\n",
      "Epoch 712/1000, Loss: 0.6932\n",
      "Epoch 713/1000, Loss: 0.6932\n",
      "Epoch 714/1000, Loss: 0.6932\n",
      "Epoch 715/1000, Loss: 0.6932\n",
      "Epoch 716/1000, Loss: 0.6932\n",
      "Epoch 717/1000, Loss: 0.6932\n",
      "Epoch 718/1000, Loss: 0.6932\n",
      "Epoch 719/1000, Loss: 0.6932\n",
      "Epoch 720/1000, Loss: 0.6932\n",
      "Epoch 721/1000, Loss: 0.6932\n",
      "Epoch 722/1000, Loss: 0.6932\n",
      "Epoch 723/1000, Loss: 0.6932\n",
      "Epoch 724/1000, Loss: 0.6932\n",
      "Epoch 725/1000, Loss: 0.6932\n",
      "Epoch 726/1000, Loss: 0.6932\n",
      "Epoch 727/1000, Loss: 0.6932\n",
      "Epoch 728/1000, Loss: 0.6932\n",
      "Epoch 729/1000, Loss: 0.6932\n",
      "Epoch 730/1000, Loss: 0.6932\n",
      "Epoch 731/1000, Loss: 0.6932\n",
      "Epoch 732/1000, Loss: 0.6932\n",
      "Epoch 733/1000, Loss: 0.6932\n",
      "Epoch 734/1000, Loss: 0.6932\n",
      "Epoch 735/1000, Loss: 0.6932\n",
      "Epoch 736/1000, Loss: 0.6932\n",
      "Epoch 737/1000, Loss: 0.6932\n",
      "Epoch 738/1000, Loss: 0.6932\n",
      "Epoch 739/1000, Loss: 0.6932\n",
      "Epoch 740/1000, Loss: 0.6932\n",
      "Epoch 741/1000, Loss: 0.6932\n",
      "Epoch 742/1000, Loss: 0.6932\n",
      "Epoch 743/1000, Loss: 0.6932\n",
      "Epoch 744/1000, Loss: 0.6932\n",
      "Epoch 745/1000, Loss: 0.6932\n",
      "Epoch 746/1000, Loss: 0.6932\n",
      "Epoch 747/1000, Loss: 0.6932\n",
      "Epoch 748/1000, Loss: 0.6932\n",
      "Epoch 749/1000, Loss: 0.6932\n",
      "Epoch 750/1000, Loss: 0.6932\n",
      "Epoch 751/1000, Loss: 0.6932\n",
      "Epoch 752/1000, Loss: 0.6932\n",
      "Epoch 753/1000, Loss: 0.6932\n",
      "Epoch 754/1000, Loss: 0.6932\n",
      "Epoch 755/1000, Loss: 0.6932\n",
      "Epoch 756/1000, Loss: 0.6932\n",
      "Epoch 757/1000, Loss: 0.6932\n",
      "Epoch 758/1000, Loss: 0.6932\n",
      "Epoch 759/1000, Loss: 0.6932\n",
      "Epoch 760/1000, Loss: 0.6932\n",
      "Epoch 761/1000, Loss: 0.6932\n",
      "Epoch 762/1000, Loss: 0.6932\n",
      "Epoch 763/1000, Loss: 0.6932\n",
      "Epoch 764/1000, Loss: 0.6932\n",
      "Epoch 765/1000, Loss: 0.6932\n",
      "Epoch 766/1000, Loss: 0.6932\n",
      "Epoch 767/1000, Loss: 0.6932\n",
      "Epoch 768/1000, Loss: 0.6932\n",
      "Epoch 769/1000, Loss: 0.6932\n",
      "Epoch 770/1000, Loss: 0.6932\n",
      "Epoch 771/1000, Loss: 0.6932\n",
      "Epoch 772/1000, Loss: 0.6932\n",
      "Epoch 773/1000, Loss: 0.6932\n",
      "Epoch 774/1000, Loss: 0.6932\n",
      "Epoch 775/1000, Loss: 0.6932\n",
      "Epoch 776/1000, Loss: 0.6932\n",
      "Epoch 777/1000, Loss: 0.6932\n",
      "Epoch 778/1000, Loss: 0.6932\n",
      "Epoch 779/1000, Loss: 0.6932\n",
      "Epoch 780/1000, Loss: 0.6932\n",
      "Epoch 781/1000, Loss: 0.6932\n",
      "Epoch 782/1000, Loss: 0.6932\n",
      "Epoch 783/1000, Loss: 0.6932\n",
      "Epoch 784/1000, Loss: 0.6932\n",
      "Epoch 785/1000, Loss: 0.6932\n",
      "Epoch 786/1000, Loss: 0.6932\n",
      "Epoch 787/1000, Loss: 0.6932\n",
      "Epoch 788/1000, Loss: 0.6932\n",
      "Epoch 789/1000, Loss: 0.6932\n",
      "Epoch 790/1000, Loss: 0.6932\n",
      "Epoch 791/1000, Loss: 0.6932\n",
      "Epoch 792/1000, Loss: 0.6932\n",
      "Epoch 793/1000, Loss: 0.6932\n",
      "Epoch 794/1000, Loss: 0.6932\n",
      "Epoch 795/1000, Loss: 0.6932\n",
      "Epoch 796/1000, Loss: 0.6932\n",
      "Epoch 797/1000, Loss: 0.6932\n",
      "Epoch 798/1000, Loss: 0.6932\n",
      "Epoch 799/1000, Loss: 0.6932\n",
      "Epoch 800/1000, Loss: 0.6932\n",
      "Epoch 801/1000, Loss: 0.6932\n",
      "Epoch 802/1000, Loss: 0.6932\n",
      "Epoch 803/1000, Loss: 0.6932\n",
      "Epoch 804/1000, Loss: 0.6932\n",
      "Epoch 805/1000, Loss: 0.6932\n",
      "Epoch 806/1000, Loss: 0.6932\n",
      "Epoch 807/1000, Loss: 0.6932\n",
      "Epoch 808/1000, Loss: 0.6932\n",
      "Epoch 809/1000, Loss: 0.6932\n",
      "Epoch 810/1000, Loss: 0.6932\n",
      "Epoch 811/1000, Loss: 0.6932\n",
      "Epoch 812/1000, Loss: 0.6932\n",
      "Epoch 813/1000, Loss: 0.6932\n",
      "Epoch 814/1000, Loss: 0.6932\n",
      "Epoch 815/1000, Loss: 0.6932\n",
      "Epoch 816/1000, Loss: 0.6932\n",
      "Epoch 817/1000, Loss: 0.6932\n",
      "Epoch 818/1000, Loss: 0.6932\n",
      "Epoch 819/1000, Loss: 0.6932\n",
      "Epoch 820/1000, Loss: 0.6932\n",
      "Epoch 821/1000, Loss: 0.6932\n",
      "Epoch 822/1000, Loss: 0.6932\n",
      "Epoch 823/1000, Loss: 0.6932\n",
      "Epoch 824/1000, Loss: 0.6932\n",
      "Epoch 825/1000, Loss: 0.6932\n",
      "Epoch 826/1000, Loss: 0.6932\n",
      "Epoch 827/1000, Loss: 0.6932\n",
      "Epoch 828/1000, Loss: 0.6932\n",
      "Epoch 829/1000, Loss: 0.6932\n",
      "Epoch 830/1000, Loss: 0.6932\n",
      "Epoch 831/1000, Loss: 0.6932\n",
      "Epoch 832/1000, Loss: 0.6932\n",
      "Epoch 833/1000, Loss: 0.6932\n",
      "Epoch 834/1000, Loss: 0.6932\n",
      "Epoch 835/1000, Loss: 0.6932\n",
      "Epoch 836/1000, Loss: 0.6932\n",
      "Epoch 837/1000, Loss: 0.6932\n",
      "Epoch 838/1000, Loss: 0.6932\n",
      "Epoch 839/1000, Loss: 0.6932\n",
      "Epoch 840/1000, Loss: 0.6932\n",
      "Epoch 841/1000, Loss: 0.6932\n",
      "Epoch 842/1000, Loss: 0.6932\n",
      "Epoch 843/1000, Loss: 0.6932\n",
      "Epoch 844/1000, Loss: 0.6931\n",
      "Epoch 845/1000, Loss: 0.6932\n",
      "Epoch 846/1000, Loss: 0.6932\n",
      "Epoch 847/1000, Loss: 0.6932\n",
      "Epoch 848/1000, Loss: 0.6932\n",
      "Epoch 849/1000, Loss: 0.6932\n",
      "Epoch 850/1000, Loss: 0.6932\n",
      "Epoch 851/1000, Loss: 0.6932\n",
      "Epoch 852/1000, Loss: 0.6932\n",
      "Epoch 853/1000, Loss: 0.6932\n",
      "Epoch 854/1000, Loss: 0.6932\n",
      "Epoch 855/1000, Loss: 0.6932\n",
      "Epoch 856/1000, Loss: 0.6932\n",
      "Epoch 857/1000, Loss: 0.6932\n",
      "Epoch 858/1000, Loss: 0.6932\n",
      "Epoch 859/1000, Loss: 0.6932\n",
      "Epoch 860/1000, Loss: 0.6932\n",
      "Epoch 861/1000, Loss: 0.6932\n",
      "Epoch 862/1000, Loss: 0.6932\n",
      "Epoch 863/1000, Loss: 0.6932\n",
      "Epoch 864/1000, Loss: 0.6932\n",
      "Epoch 865/1000, Loss: 0.6932\n",
      "Epoch 866/1000, Loss: 0.6932\n",
      "Epoch 867/1000, Loss: 0.6932\n",
      "Epoch 868/1000, Loss: 0.6932\n",
      "Epoch 869/1000, Loss: 0.6932\n",
      "Epoch 870/1000, Loss: 0.6932\n",
      "Epoch 871/1000, Loss: 0.6932\n",
      "Epoch 872/1000, Loss: 0.6932\n",
      "Epoch 873/1000, Loss: 0.6932\n",
      "Epoch 874/1000, Loss: 0.6932\n",
      "Epoch 875/1000, Loss: 0.6932\n",
      "Epoch 876/1000, Loss: 0.6932\n",
      "Epoch 877/1000, Loss: 0.6932\n",
      "Epoch 878/1000, Loss: 0.6932\n",
      "Epoch 879/1000, Loss: 0.6932\n",
      "Epoch 880/1000, Loss: 0.6932\n",
      "Epoch 881/1000, Loss: 0.6932\n",
      "Epoch 882/1000, Loss: 0.6932\n",
      "Epoch 883/1000, Loss: 0.6932\n",
      "Epoch 884/1000, Loss: 0.6932\n",
      "Epoch 885/1000, Loss: 0.6932\n",
      "Epoch 886/1000, Loss: 0.6932\n",
      "Epoch 887/1000, Loss: 0.6932\n",
      "Epoch 888/1000, Loss: 0.6932\n",
      "Epoch 889/1000, Loss: 0.6932\n",
      "Epoch 890/1000, Loss: 0.6932\n",
      "Epoch 891/1000, Loss: 0.6932\n",
      "Epoch 892/1000, Loss: 0.6932\n",
      "Epoch 893/1000, Loss: 0.6932\n",
      "Epoch 894/1000, Loss: 0.6932\n",
      "Epoch 895/1000, Loss: 0.6932\n",
      "Epoch 896/1000, Loss: 0.6932\n",
      "Epoch 897/1000, Loss: 0.6932\n",
      "Epoch 898/1000, Loss: 0.6932\n",
      "Epoch 899/1000, Loss: 0.6932\n",
      "Epoch 900/1000, Loss: 0.6932\n",
      "Epoch 901/1000, Loss: 0.6932\n",
      "Epoch 902/1000, Loss: 0.6932\n",
      "Epoch 903/1000, Loss: 0.6932\n",
      "Epoch 904/1000, Loss: 0.6932\n",
      "Epoch 905/1000, Loss: 0.6932\n",
      "Epoch 906/1000, Loss: 0.6932\n",
      "Epoch 907/1000, Loss: 0.6932\n",
      "Epoch 908/1000, Loss: 0.6932\n",
      "Epoch 909/1000, Loss: 0.6932\n",
      "Epoch 910/1000, Loss: 0.6932\n",
      "Epoch 911/1000, Loss: 0.6932\n",
      "Epoch 912/1000, Loss: 0.6932\n",
      "Epoch 913/1000, Loss: 0.6932\n",
      "Epoch 914/1000, Loss: 0.6932\n",
      "Epoch 915/1000, Loss: 0.6932\n",
      "Epoch 916/1000, Loss: 0.6932\n",
      "Epoch 917/1000, Loss: 0.6932\n",
      "Epoch 918/1000, Loss: 0.6932\n",
      "Epoch 919/1000, Loss: 0.6932\n",
      "Epoch 920/1000, Loss: 0.6932\n",
      "Epoch 921/1000, Loss: 0.6932\n",
      "Epoch 922/1000, Loss: 0.6932\n",
      "Epoch 923/1000, Loss: 0.6932\n",
      "Epoch 924/1000, Loss: 0.6932\n",
      "Epoch 925/1000, Loss: 0.6932\n",
      "Epoch 926/1000, Loss: 0.6932\n",
      "Epoch 927/1000, Loss: 0.6932\n",
      "Epoch 928/1000, Loss: 0.6932\n",
      "Epoch 929/1000, Loss: 0.6932\n",
      "Epoch 930/1000, Loss: 0.6932\n",
      "Epoch 931/1000, Loss: 0.6932\n",
      "Epoch 932/1000, Loss: 0.6932\n",
      "Epoch 933/1000, Loss: 0.6932\n",
      "Epoch 934/1000, Loss: 0.6932\n",
      "Epoch 935/1000, Loss: 0.6932\n",
      "Epoch 936/1000, Loss: 0.6932\n",
      "Epoch 937/1000, Loss: 0.6932\n",
      "Epoch 938/1000, Loss: 0.6932\n",
      "Epoch 939/1000, Loss: 0.6932\n",
      "Epoch 940/1000, Loss: 0.6932\n",
      "Epoch 941/1000, Loss: 0.6932\n",
      "Epoch 942/1000, Loss: 0.6932\n",
      "Epoch 943/1000, Loss: 0.6932\n",
      "Epoch 944/1000, Loss: 0.6932\n",
      "Epoch 945/1000, Loss: 0.6932\n",
      "Epoch 946/1000, Loss: 0.6932\n",
      "Epoch 947/1000, Loss: 0.6932\n",
      "Epoch 948/1000, Loss: 0.6932\n",
      "Epoch 949/1000, Loss: 0.6932\n",
      "Epoch 950/1000, Loss: 0.6932\n",
      "Epoch 951/1000, Loss: 0.6932\n",
      "Epoch 952/1000, Loss: 0.6932\n",
      "Epoch 953/1000, Loss: 0.6932\n",
      "Epoch 954/1000, Loss: 0.6932\n",
      "Epoch 955/1000, Loss: 0.6932\n",
      "Epoch 956/1000, Loss: 0.6932\n",
      "Epoch 957/1000, Loss: 0.6932\n",
      "Epoch 958/1000, Loss: 0.6932\n",
      "Epoch 959/1000, Loss: 0.6932\n",
      "Epoch 960/1000, Loss: 0.6932\n",
      "Epoch 961/1000, Loss: 0.6932\n",
      "Epoch 962/1000, Loss: 0.6932\n",
      "Epoch 963/1000, Loss: 0.6932\n",
      "Epoch 964/1000, Loss: 0.6932\n",
      "Epoch 965/1000, Loss: 0.6932\n",
      "Epoch 966/1000, Loss: 0.6932\n",
      "Epoch 967/1000, Loss: 0.6932\n",
      "Epoch 968/1000, Loss: 0.6932\n",
      "Epoch 969/1000, Loss: 0.6932\n",
      "Epoch 970/1000, Loss: 0.6932\n",
      "Epoch 971/1000, Loss: 0.6932\n",
      "Epoch 972/1000, Loss: 0.6932\n",
      "Epoch 973/1000, Loss: 0.6932\n",
      "Epoch 974/1000, Loss: 0.6932\n",
      "Epoch 975/1000, Loss: 0.6932\n",
      "Epoch 976/1000, Loss: 0.6932\n",
      "Epoch 977/1000, Loss: 0.6932\n",
      "Epoch 978/1000, Loss: 0.6932\n",
      "Epoch 979/1000, Loss: 0.6932\n",
      "Epoch 980/1000, Loss: 0.6932\n",
      "Epoch 981/1000, Loss: 0.6932\n",
      "Epoch 982/1000, Loss: 0.6932\n",
      "Epoch 983/1000, Loss: 0.6932\n",
      "Epoch 984/1000, Loss: 0.6932\n",
      "Epoch 985/1000, Loss: 0.6932\n",
      "Epoch 986/1000, Loss: 0.6932\n",
      "Epoch 987/1000, Loss: 0.6932\n",
      "Epoch 988/1000, Loss: 0.6932\n",
      "Epoch 989/1000, Loss: 0.6932\n",
      "Epoch 990/1000, Loss: 0.6932\n",
      "Epoch 991/1000, Loss: 0.6932\n",
      "Epoch 992/1000, Loss: 0.6932\n",
      "Epoch 993/1000, Loss: 0.6932\n",
      "Epoch 994/1000, Loss: 0.6932\n",
      "Epoch 995/1000, Loss: 0.6932\n",
      "Epoch 996/1000, Loss: 0.6932\n",
      "Epoch 997/1000, Loss: 0.6932\n",
      "Epoch 998/1000, Loss: 0.6932\n",
      "Epoch 999/1000, Loss: 0.6932\n",
      "Epoch 1000/1000, Loss: 0.6932\n"
     ]
    }
   ],
   "source": [
    "class DeepNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 4096)\n",
    "        self.layer2 = nn.Linear(4096, 2048)\n",
    "        self.layer3 = nn.Linear(2048, 1024)\n",
    "        self.layer4 = nn.Linear(1024, 512)\n",
    "        self.layer5 = nn.Linear(512, 256)\n",
    "        self.layer6 = nn.Linear(256, 128)\n",
    "        self.layer7 = nn.Linear(128, 64)\n",
    "        self.layer8 = nn.Linear(64, 32)\n",
    "        self.layer9 = nn.Linear(32, 16)\n",
    "        self.layer10 = nn.Linear(16, 8)\n",
    "        self.layer11 = nn.Linear(8, 4)\n",
    "        self.layer12 = nn.Linear(4, 1)  # Adjusted to match the expected output size\n",
    "        self.activation = nn.ReLU()\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.layer1(x))\n",
    "        x = self.activation(self.layer2(x))\n",
    "        x = self.activation(self.layer3(x))\n",
    "        x = self.activation(self.layer4(x))\n",
    "        x = self.activation(self.layer5(x))\n",
    "        x = self.activation(self.layer6(x))\n",
    "        x = self.activation(self.layer7(x))\n",
    "        x = self.activation(self.layer8(x))\n",
    "        x = self.activation(self.layer9(x))\n",
    "        x = self.activation(self.layer10(x))\n",
    "        x = self.activation(self.layer11(x))\n",
    "        x = self.output_activation(self.layer12(x))\n",
    "        return x\n",
    "\n",
    "input_dim = 145\n",
    "deep_model = DeepNet(input_dim)\n",
    "\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer_unique = optim.Adam(deep_model.parameters(), lr=0.001)\n",
    "\n",
    "total_epochs = 1000\n",
    "deep_model.train()\n",
    "for epoch_num in range(total_epochs):\n",
    "    accumulated_loss = 0.0\n",
    "    for batch_features, batch_labels in train_loader_unique:\n",
    "        optimizer_unique.zero_grad()\n",
    "        batch_outputs = deep_model(batch_features)\n",
    "        batch_loss = loss_function(batch_outputs, batch_labels)\n",
    "        batch_loss.backward()\n",
    "        optimizer_unique.step()\n",
    "\n",
    "        accumulated_loss += batch_loss.item() * batch_features.size(0)\n",
    "\n",
    "    avg_epoch_loss = accumulated_loss / len(train_loader_unique.dataset)\n",
    "    print(f'Epoch {epoch_num+1}/{total_epochs}, Loss: {avg_epoch_loss:.4f}')\n",
    "\n",
    "deep_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = deep_model(test_features)\n",
    "    test_predictions = (test_outputs.numpy() > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6406769416742218\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kisha\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#evaluating the model\n",
    "print('Accuracy:', accuracy_score(y_test, test_predictions))\n",
    "print('Precision:', precision_score(y_test, test_predictions))\n",
    "print('Recall:', recall_score(y_test, test_predictions))\n",
    "print('F1 Score:', f1_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#confusion matrix plot\n",
    "\n",
    "cm = confusion_matrix(y_test, test_predictions)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_predict(input_data):\n",
    "#     input_tensor = torch.tensor(input_data, dtype=torch.float32)\n",
    "#     deep_model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         predictions = deep_model(input_tensor).numpy()\n",
    "#     return predictions\n",
    "\n",
    "# # Convert the train features to a numpy array (if not already done)\n",
    "# train_features_np = train_features.numpy()\n",
    "\n",
    "# # Initialize the SHAP explainer\n",
    "# explainer = shap.Explainer(model_predict, train_features_np)\n",
    "\n",
    "# # Calculate SHAP values\n",
    "# shap_values = explainer(train_features_np)\n",
    "\n",
    "# # Plot the SHAP values\n",
    "# shap.summary_plot(shap_values, train_features_np, feature_names=X.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
